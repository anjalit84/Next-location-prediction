{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GNN Training on 10 Users' Trajectories\n",
    "\n",
    "This notebook:\n",
    "- Loads 10 specific users' trajectories: ['000', '001', '005', '006', '009', '011', '014', '016', '019', '025']\n",
    "- Removes consecutive duplicates (AAABCDCCABB → ABCDCAB) for each user\n",
    "- Builds graph structure (nodes = places, edges = transitions)\n",
    "- Trains GNN model (GCN + LSTM) for location prediction\n",
    "- Evaluates all 4 metrics: Accuracy, Precision & Recall, Top-K Accuracy, MPD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1 — Imports & Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from math import radians, sin, cos, sqrt, atan2\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv, SAGEConv, GATConv\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Manual haversine implementation to avoid dependency issues\n",
    "def haversine(coord1, coord2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two points on Earth.\n",
    "    \n",
    "    Parameters:\n",
    "    coord1: tuple of (latitude, longitude) in degrees\n",
    "    coord2: tuple of (latitude, longitude) in degrees\n",
    "    \n",
    "    Returns:\n",
    "    Distance in kilometers\n",
    "    \"\"\"\n",
    "    lat1, lon1 = coord1\n",
    "    lat2, lon2 = coord2\n",
    "    \n",
    "    # Convert to radians\n",
    "    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n",
    "    \n",
    "    # Haversine formula\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1-a))\n",
    "    \n",
    "    # Earth radius in kilometers\n",
    "    R = 6371.0\n",
    "    \n",
    "    return R * c\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "# Paths\n",
    "BASE_PATH = \"/home/root495/Inexture/Location Prediction Update\"\n",
    "PROCESSED_PATH = BASE_PATH + \"/data/processed/\"\n",
    "SEQUENCES_FILE = PROCESSED_PATH + \"place_sequences.json\"\n",
    "GRAPH_EDGES_FILE = PROCESSED_PATH + \"graph_edges.csv\"\n",
    "NODE_FEATURES_FILE = PROCESSED_PATH + \"node_features.csv\"\n",
    "GRID_METADATA_FILE = PROCESSED_PATH + \"grid_metadata.json\"\n",
    "CLEANED_WITH_PLACES_FILE = PROCESSED_PATH + \"cleaned_with_places.csv\"\n",
    "OUTPUT_PATH = BASE_PATH + \"/notebooks/\"\n",
    "MODELS_PATH = BASE_PATH + \"/models/\"\n",
    "RESULTS_PATH = BASE_PATH + \"/results/\"\n",
    "MODEL_SAVE_PATH = MODELS_PATH + \"gnn_10users_model_best.pt\"\n",
    "RESULTS_SAVE_PATH = RESULTS_PATH + \"gnn_10users_results.json\"\n",
    "\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "os.makedirs(MODELS_PATH, exist_ok=True)\n",
    "os.makedirs(RESULTS_PATH, exist_ok=True)\n",
    "\n",
    "# Specific users to use\n",
    "SELECTED_USERS = ['000', '001', '005', '006', '009', '011', '014', '016', '019', '025']\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Optimize for speed\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.benchmark = True  # Faster convolutions\n",
    "    torch.backends.cudnn.deterministic = False  # Allow non-deterministic for speed\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2 — Load 10 Users' Trajectories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading place sequences...\n",
      "Total users available: 54\n",
      "  User 000: 173817 places\n",
      "  User 001: 108561 places\n",
      "  User 005: 108967 places\n",
      "  User 006: 31809 places\n",
      "  User 009: 84573 places\n",
      "  User 011: 90770 places\n",
      "  User 014: 388051 places\n",
      "  User 016: 89208 places\n",
      "  User 019: 47792 places\n",
      "  User 025: 628816 places\n",
      "\n",
      "Selected 10 users: ['000', '001', '005', '006', '009', '011', '014', '016', '019', '025']\n",
      "Total places across all users: 1752364\n"
     ]
    }
   ],
   "source": [
    "# Load place sequences\n",
    "print(\"Loading place sequences...\")\n",
    "with open(SEQUENCES_FILE, 'r') as f:\n",
    "    sequences_dict = json.load(f)\n",
    "\n",
    "print(f\"Total users available: {len(sequences_dict)}\")\n",
    "\n",
    "# Load sequences for specific users\n",
    "user_sequences = {}\n",
    "total_places = 0\n",
    "for user_id in SELECTED_USERS:\n",
    "    if user_id in sequences_dict:\n",
    "        seq = sequences_dict[user_id]\n",
    "        user_sequences[user_id] = seq\n",
    "        total_places += len(seq)\n",
    "        print(f\"  User {user_id}: {len(seq)} places\")\n",
    "    else:\n",
    "        print(f\"  Warning: User {user_id} not found in sequences!\")\n",
    "\n",
    "print(f\"\\nSelected {len(user_sequences)} users: {list(user_sequences.keys())}\")\n",
    "print(f\"Total places across all users: {total_places}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3 — Preprocess: Remove Consecutive Duplicates\n",
    "\n",
    "Remove consecutive duplicate locations for each user. Example: AAABCDCCABB → ABCDCAB\n",
    "\n",
    "Only consecutive duplicates are removed. If a location appears again later (non-consecutive), it is kept.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing users...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Removing duplicates:  70%|███████   | 7/10 [00:00<00:00, 47.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  User 000: 173817 → 795 places (99.5% reduction)\n",
      "  User 001: 108561 → 186 places (99.8% reduction)\n",
      "  User 005: 108967 → 283 places (99.7% reduction)\n",
      "  User 006: 31809 → 103 places (99.7% reduction)\n",
      "  User 009: 84573 → 17 places (100.0% reduction)\n",
      "  User 011: 90770 → 125 places (99.9% reduction)\n",
      "  User 014: 388051 → 766 places (99.8% reduction)\n",
      "  User 016: 89208 → 124 places (99.9% reduction)\n",
      "  User 019: 47792 → 120 places (99.7% reduction)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Removing duplicates: 100%|██████████| 10/10 [00:00<00:00, 42.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  User 025: 628816 → 1568 places (99.8% reduction)\n",
      "\n",
      "Summary:\n",
      "  Total original places: 1752364\n",
      "  Total after processing: 4087\n",
      "  Total duplicates removed: 1748277 (99.8%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def remove_consecutive_duplicates(sequence):\n",
    "    \"\"\"\n",
    "    Remove consecutive duplicates from sequence.\n",
    "    Example: [A, A, A, B, C, D, C, C, A, B, B] → [A, B, C, D, C, A, B]\n",
    "    \"\"\"\n",
    "    if len(sequence) == 0:\n",
    "        return sequence\n",
    "    \n",
    "    processed = [sequence[0]]  # Always keep first element\n",
    "    \n",
    "    for i in range(1, len(sequence)):\n",
    "        # Only add if different from previous (not consecutive duplicate)\n",
    "        if sequence[i] != sequence[i-1]:\n",
    "            processed.append(sequence[i])\n",
    "    \n",
    "    return processed\n",
    "\n",
    "# Apply consecutive duplicate removal to each user\n",
    "processed_sequences = {}\n",
    "total_original = 0\n",
    "total_processed = 0\n",
    "\n",
    "print(\"Processing users...\")\n",
    "for user_id in tqdm(list(user_sequences.keys()), desc=\"Removing duplicates\"):\n",
    "    original_seq = user_sequences[user_id]\n",
    "    processed_seq = remove_consecutive_duplicates(original_seq)\n",
    "    processed_sequences[user_id] = processed_seq\n",
    "    \n",
    "    original_len = len(original_seq)\n",
    "    processed_len = len(processed_seq)\n",
    "    total_original += original_len\n",
    "    total_processed += processed_len\n",
    "    \n",
    "    reduction = original_len - processed_len\n",
    "    reduction_pct = (reduction / original_len * 100) if original_len > 0 else 0\n",
    "    print(f\"  User {user_id}: {original_len} → {processed_len} places ({reduction_pct:.1f}% reduction)\")\n",
    "\n",
    "print(f\"\\nSummary:\")\n",
    "print(f\"  Total original places: {total_original}\")\n",
    "print(f\"  Total after processing: {total_processed}\")\n",
    "print(f\"  Total duplicates removed: {total_original - total_processed} ({((total_original - total_processed)/total_original*100):.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4 — Build Graph Structure\n",
    "\n",
    "Build graph from trajectory data:\n",
    "- Nodes = unique place IDs\n",
    "- Edges = transitions between places\n",
    "- Edge weights = transition frequencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building graph structure...\n",
      "Total unique places (nodes): 303\n",
      "Total edges: 690\n",
      "Edge index shape: torch.Size([2, 690])\n",
      "Graph built successfully!\n"
     ]
    }
   ],
   "source": [
    "# Build graph from sequences\n",
    "print(\"Building graph structure...\")\n",
    "\n",
    "# Collect all unique places\n",
    "all_places = set()\n",
    "for seq in processed_sequences.values():\n",
    "    all_places.update(seq)\n",
    "\n",
    "# Create place to index mapping\n",
    "place_to_idx = {place: idx for idx, place in enumerate(sorted(all_places))}\n",
    "idx_to_place = {idx: place for place, idx in place_to_idx.items()}\n",
    "num_nodes = len(place_to_idx)\n",
    "\n",
    "print(f\"Total unique places (nodes): {num_nodes}\")\n",
    "\n",
    "# Build edge list and weights from sequences\n",
    "edge_index = []\n",
    "edge_weights = []\n",
    "transition_counts = {}\n",
    "\n",
    "for seq in processed_sequences.values():\n",
    "    for i in range(len(seq) - 1):\n",
    "        source = seq[i]\n",
    "        target = seq[i+1]\n",
    "        \n",
    "        source_idx = place_to_idx[source]\n",
    "        target_idx = place_to_idx[target]\n",
    "        \n",
    "        # Count transitions\n",
    "        if (source_idx, target_idx) not in transition_counts:\n",
    "            transition_counts[(source_idx, target_idx)] = 0\n",
    "        transition_counts[(source_idx, target_idx)] += 1\n",
    "\n",
    "# Create edge list and weights\n",
    "for (source_idx, target_idx), count in transition_counts.items():\n",
    "    edge_index.append([source_idx, target_idx])\n",
    "    edge_weights.append(count)\n",
    "\n",
    "edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "edge_weights = torch.tensor(edge_weights, dtype=torch.float)\n",
    "\n",
    "print(f\"Total edges: {len(edge_weights)}\")\n",
    "print(f\"Edge index shape: {edge_index.shape}\")\n",
    "print(f\"Graph built successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5 — Create Sequences for Training\n",
    "\n",
    "Split each user's processed sequence into fixed-length chunks of 50 events each.\n",
    "Combine sequences from all users for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating sequences from all users...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing users: 100%|██████████| 10/10 [00:00<00:00, 19991.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  User 000: 30 sequences\n",
      "  User 001: 6 sequences\n",
      "  User 005: 10 sequences\n",
      "  User 006: 3 sequences\n",
      "  User 009: 0 sequences\n",
      "  User 011: 4 sequences\n",
      "  User 014: 29 sequences\n",
      "  User 016: 3 sequences\n",
      "  User 019: 3 sequences\n",
      "  User 025: 61 sequences\n",
      "\n",
      "Total sequences created: 149\n",
      "Total events in sequences: 7450\n",
      "\n",
      "Training sequences: 119\n",
      "Test sequences: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create sequences of fixed length 50\n",
    "SEQUENCE_LENGTH = 50\n",
    "\n",
    "# Use sliding windows for more training data (balanced overlap)\n",
    "# Create overlapping sequences with step size of 25 (50% overlap - good for learning)\n",
    "all_sequences = []\n",
    "step_size = 25  # 50% overlap for better learning\n",
    "\n",
    "print(\"Creating sequences from all users...\")\n",
    "for user_id in tqdm(list(processed_sequences.keys()), desc=\"Processing users\"):\n",
    "    processed_seq = processed_sequences[user_id]\n",
    "    user_sequences_list = []\n",
    "    \n",
    "    for i in range(0, len(processed_seq) - SEQUENCE_LENGTH + 1, step_size):\n",
    "        chunk = processed_seq[i:i+SEQUENCE_LENGTH]\n",
    "        if len(chunk) == SEQUENCE_LENGTH:  # Only full-length sequences\n",
    "            user_sequences_list.append(chunk)\n",
    "    \n",
    "    all_sequences.extend(user_sequences_list)\n",
    "    print(f\"  User {user_id}: {len(user_sequences_list)} sequences\")\n",
    "\n",
    "print(f\"\\nTotal sequences created: {len(all_sequences)}\")\n",
    "print(f\"Total events in sequences: {sum(len(s) for s in all_sequences)}\")\n",
    "\n",
    "# Split into train/test (80/20)\n",
    "split_idx = int(len(all_sequences) * 0.8)\n",
    "train_sequences = all_sequences[:split_idx]\n",
    "test_sequences = all_sequences[split_idx:]\n",
    "\n",
    "print(f\"\\nTraining sequences: {len(train_sequences)}\")\n",
    "print(f\"Test sequences: {len(test_sequences)}\")\n",
    "\n",
    "if len(test_sequences) == 0:\n",
    "    # If no test sequences, use last training sequence for testing\n",
    "    test_sequences = [train_sequences[-1]]\n",
    "    train_sequences = train_sequences[:-1]\n",
    "    print(f\"Adjusted: Training={len(train_sequences)}, Test=1 (using last training sequence)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6 — Prepare Node Features\n",
    "\n",
    "Extract node features for each place:\n",
    "- Visit frequency\n",
    "- Coordinates (lat, lon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading coordinates...\n",
      "Calculating visit frequencies...\n",
      "Calculating node degrees...\n",
      "Collecting coordinates for normalization...\n",
      "Coordinate ranges: lat=[18.2817, 41.1173], lon=[109.1517, 121.8167]\n",
      "Node features shape: torch.Size([303, 6])\n",
      "Node features: [visit_frequency, lat, lon, in_degree, out_degree, max_transition_prob]\n"
     ]
    }
   ],
   "source": [
    "# Load coordinates\n",
    "print(\"Loading coordinates...\")\n",
    "df_places = pd.read_csv(CLEANED_WITH_PLACES_FILE)\n",
    "place_coords = df_places.groupby('place_id')[['lat', 'lon']].first().to_dict('index')\n",
    "\n",
    "# Load grid metadata for fallback\n",
    "with open(GRID_METADATA_FILE, 'r') as f:\n",
    "    grid_metadata = json.load(f)\n",
    "\n",
    "# Calculate visit frequency for each place\n",
    "print(\"Calculating visit frequencies...\")\n",
    "place_visit_counts = {}\n",
    "for seq in train_sequences + test_sequences:\n",
    "    for place in seq:\n",
    "        place_visit_counts[place] = place_visit_counts.get(place, 0) + 1\n",
    "\n",
    "# Calculate in-degree and out-degree for each node\n",
    "print(\"Calculating node degrees...\")\n",
    "in_degree = {idx: 0 for idx in range(num_nodes)}\n",
    "out_degree = {idx: 0 for idx in range(num_nodes)}\n",
    "transition_freq = {idx: {} for idx in range(num_nodes)}\n",
    "\n",
    "for seq in train_sequences + test_sequences:\n",
    "    indices = [place_to_idx[place] for place in seq]\n",
    "    for i in range(len(indices) - 1):\n",
    "        source_idx = indices[i]\n",
    "        target_idx = indices[i+1]\n",
    "        out_degree[source_idx] += 1\n",
    "        in_degree[target_idx] += 1\n",
    "        if target_idx not in transition_freq[source_idx]:\n",
    "            transition_freq[source_idx][target_idx] = 0\n",
    "        transition_freq[source_idx][target_idx] += 1\n",
    "\n",
    "# Calculate transition probability (max transition prob from this node)\n",
    "max_transition_prob = {}\n",
    "for source_idx in range(num_nodes):\n",
    "    if len(transition_freq[source_idx]) > 0:\n",
    "        total = sum(transition_freq[source_idx].values())\n",
    "        max_prob = max(transition_freq[source_idx].values()) / total if total > 0 else 0.0\n",
    "        max_transition_prob[source_idx] = max_prob\n",
    "    else:\n",
    "        max_transition_prob[source_idx] = 0.0\n",
    "\n",
    "# First, collect all coordinates to calculate min/max for normalization\n",
    "print(\"Collecting coordinates for normalization...\")\n",
    "all_lats = []\n",
    "all_lons = []\n",
    "for idx in range(num_nodes):\n",
    "    place_id = idx_to_place[idx]\n",
    "    if place_id in place_coords:\n",
    "        all_lats.append(place_coords[place_id]['lat'])\n",
    "        all_lons.append(place_coords[place_id]['lon'])\n",
    "    else:\n",
    "        # Fallback: calculate from grid\n",
    "        try:\n",
    "            if \"_\" in str(place_id):\n",
    "                row, col = map(int, str(place_id).split(\"_\"))\n",
    "                lat = grid_metadata['min_lat'] + row * grid_metadata['deg_lat']\n",
    "                lon = grid_metadata['min_lon'] + col * grid_metadata['deg_lon']\n",
    "                all_lats.append(lat)\n",
    "                all_lons.append(lon)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# Calculate min/max for normalization\n",
    "min_lat = min(all_lats) if all_lats else grid_metadata['min_lat']\n",
    "max_lat = max(all_lats) if all_lats else grid_metadata['min_lat'] + 100 * grid_metadata['deg_lat']\n",
    "min_lon = min(all_lons) if all_lons else grid_metadata['min_lon']\n",
    "max_lon = max(all_lons) if all_lons else grid_metadata['min_lon'] + 100 * grid_metadata['deg_lon']\n",
    "\n",
    "print(f\"Coordinate ranges: lat=[{min_lat:.4f}, {max_lat:.4f}], lon=[{min_lon:.4f}, {max_lon:.4f}]\")\n",
    "\n",
    "# Normalize degrees\n",
    "max_in_degree = max(in_degree.values()) if in_degree.values() else 1\n",
    "max_out_degree = max(out_degree.values()) if out_degree.values() else 1\n",
    "\n",
    "# Build enhanced node features\n",
    "node_features = []\n",
    "for idx in range(num_nodes):\n",
    "    place_id = idx_to_place[idx]\n",
    "    \n",
    "    # Feature 1: Visit frequency (normalized)\n",
    "    visit_freq = place_visit_counts.get(place_id, 0)\n",
    "    max_visits = max(place_visit_counts.values()) if place_visit_counts else 1\n",
    "    visit_freq_norm = visit_freq / max_visits if max_visits > 0 else 0.0\n",
    "    \n",
    "    # Feature 2 & 3: Coordinates\n",
    "    if place_id in place_coords:\n",
    "        lat = place_coords[place_id]['lat']\n",
    "        lon = place_coords[place_id]['lon']\n",
    "    else:\n",
    "        # Fallback: calculate from grid\n",
    "        try:\n",
    "            if \"_\" in str(place_id):\n",
    "                row, col = map(int, str(place_id).split(\"_\"))\n",
    "                lat = grid_metadata['min_lat'] + row * grid_metadata['deg_lat']\n",
    "                lon = grid_metadata['min_lon'] + col * grid_metadata['deg_lon']\n",
    "            else:\n",
    "                lat, lon = min_lat, min_lon  # Use min as default\n",
    "        except:\n",
    "            lat, lon = min_lat, min_lon\n",
    "    \n",
    "    # Normalize coordinates\n",
    "    lat_norm = (lat - min_lat) / (max_lat - min_lat + 1e-8)\n",
    "    lon_norm = (lon - min_lon) / (max_lon - min_lon + 1e-8)\n",
    "    \n",
    "    # Feature 4: In-degree (normalized)\n",
    "    in_deg_norm = in_degree[idx] / max_in_degree if max_in_degree > 0 else 0.0\n",
    "    \n",
    "    # Feature 5: Out-degree (normalized)\n",
    "    out_deg_norm = out_degree[idx] / max_out_degree if max_out_degree > 0 else 0.0\n",
    "    \n",
    "    # Feature 6: Max transition probability\n",
    "    max_trans_prob = max_transition_prob[idx]\n",
    "    \n",
    "    node_features.append([visit_freq_norm, lat_norm, lon_norm, in_deg_norm, out_deg_norm, max_trans_prob])\n",
    "\n",
    "node_features = torch.tensor(node_features, dtype=torch.float)\n",
    "print(f\"Node features shape: {node_features.shape}\")\n",
    "print(f\"Node features: [visit_frequency, lat, lon, in_degree, out_degree, max_transition_prob]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7 — Train GNN Model\n",
    "\n",
    "Define and train GCN + LSTM model for sequence prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized (optimized for speed and performance):\n",
      "  Nodes: 303\n",
      "  Node feature dim: 6\n",
      "  Hidden dim: 200\n",
      "  GNN layers: 3 (GraphSAGE), LSTM layers: 2 (Bidirectional)\n",
      "  Parameters: 899,503\n"
     ]
    }
   ],
   "source": [
    "# Define enhanced GNN + LSTM model (optimized for best performance)\n",
    "class ImprovedGNNLSTM(nn.Module):\n",
    "    def __init__(self, num_nodes, node_feature_dim, hidden_dim=224, gnn_layers=4, lstm_layers=2, dropout=0.2):\n",
    "        super(ImprovedGNNLSTM, self).__init__()\n",
    "        \n",
    "        self.num_nodes = num_nodes\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # GraphSAGE layers (better embeddings than GCN)\n",
    "        self.gnn_layers = nn.ModuleList()\n",
    "        self.bn_layers = nn.ModuleList()\n",
    "        \n",
    "        # First layer: node features -> hidden\n",
    "        self.gnn_layers.append(SAGEConv(node_feature_dim, hidden_dim))\n",
    "        self.bn_layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "        \n",
    "        # Additional GraphSAGE layers (increased to 4)\n",
    "        for _ in range(gnn_layers - 1):\n",
    "            self.gnn_layers.append(SAGEConv(hidden_dim, hidden_dim))\n",
    "            self.bn_layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "        \n",
    "        # Bidirectional LSTM (important for sequence understanding)\n",
    "        lstm_hidden = hidden_dim // 2  # Use half hidden for each direction\n",
    "        self.lstm = nn.LSTM(hidden_dim, lstm_hidden, lstm_layers, \n",
    "                           batch_first=True, dropout=dropout if lstm_layers > 1 else 0,\n",
    "                           bidirectional=True)  # Bidirectional for better context\n",
    "        # Output will be hidden_dim (lstm_hidden * 2)\n",
    "        \n",
    "        # Multi-head attention mechanism (increased heads)\n",
    "        self.attention = nn.MultiheadAttention(hidden_dim, num_heads=8, dropout=dropout, batch_first=True)\n",
    "        \n",
    "        # Enhanced output layers with more capacity\n",
    "        self.fc1 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        self.fc3 = nn.Linear(hidden_dim // 2, num_nodes)\n",
    "        self.dropout_layer = nn.Dropout(dropout)\n",
    "        self.dropout_layer2 = nn.Dropout(dropout * 0.3)  # Even lighter dropout for output\n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim)\n",
    "        \n",
    "        # Initialize weights better\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights for better training\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "        \n",
    "    def forward(self, x, edge_index, sequence_indices):\n",
    "        \"\"\"\n",
    "        x: node features [num_nodes, node_feature_dim]\n",
    "        edge_index: edge indices [2, num_edges]\n",
    "        sequence_indices: list of node indices in sequence [batch_size, seq_len]\n",
    "        \"\"\"\n",
    "        # Get node embeddings from GraphSAGE with batch norm\n",
    "        h = x\n",
    "        for i, (gnn_layer, bn_layer) in enumerate(zip(self.gnn_layers, self.bn_layers)):\n",
    "            h = gnn_layer(h, edge_index)\n",
    "            h = bn_layer(h)\n",
    "            h = torch.relu(h)\n",
    "            if i < len(self.gnn_layers) - 1:  # Apply dropout except on last layer\n",
    "                h = nn.functional.dropout(h, p=self.dropout, training=self.training)\n",
    "        \n",
    "        # Get embeddings for sequence nodes\n",
    "        batch_size, seq_len = sequence_indices.shape\n",
    "        sequence_embeddings = h[sequence_indices]  # [batch_size, seq_len, hidden_dim]\n",
    "        \n",
    "        # Process through bidirectional LSTM\n",
    "        lstm_out, _ = self.lstm(sequence_embeddings)  # [batch_size, seq_len, hidden_dim] (bidirectional output)\n",
    "        \n",
    "        # Apply layer norm before attention\n",
    "        lstm_out = self.layer_norm(lstm_out)\n",
    "        \n",
    "        # Apply multi-head attention\n",
    "        attn_out, _ = self.attention(lstm_out, lstm_out, lstm_out)  # [batch_size, seq_len, hidden_dim]\n",
    "        \n",
    "        # Use weighted combination of last few hidden states (better than just last)\n",
    "        # Weight recent states more heavily\n",
    "        seq_len = attn_out.shape[1]\n",
    "        if seq_len >= 5:\n",
    "            # Weighted average of last 5 states (more context)\n",
    "            weights = torch.tensor([0.1, 0.15, 0.2, 0.25, 0.3], device=attn_out.device).view(1, 5, 1)\n",
    "            last_hidden = (attn_out[:, -5:, :] * weights).sum(dim=1)  # [batch_size, hidden_dim]\n",
    "        elif seq_len >= 3:\n",
    "            # Weighted average of last 3 states\n",
    "            weights = torch.tensor([0.2, 0.3, 0.5], device=attn_out.device).view(1, 3, 1)\n",
    "            last_hidden = (attn_out[:, -3:, :] * weights).sum(dim=1)  # [batch_size, hidden_dim]\n",
    "        else:\n",
    "            last_hidden = attn_out[:, -1, :]  # [batch_size, hidden_dim]\n",
    "        \n",
    "        # Predict next location through enhanced FC layers\n",
    "        output = self.fc1(last_hidden)\n",
    "        output = torch.relu(output)\n",
    "        output = self.dropout_layer(output)\n",
    "        output = self.fc2(output)\n",
    "        output = torch.relu(output)\n",
    "        output = self.dropout_layer2(output)  # Lighter dropout before final layer\n",
    "        output = self.fc3(output)  # [batch_size, num_nodes]\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Initialize optimized model (balanced for speed and performance)\n",
    "node_feature_dim = node_features.shape[1]\n",
    "hidden_dim = 200  # Reduced for faster training while maintaining performance\n",
    "model = ImprovedGNNLSTM(num_nodes, node_feature_dim, hidden_dim=hidden_dim, \n",
    "                        gnn_layers=3, lstm_layers=2, dropout=0.2).to(device)  # Reduced GNN layers\n",
    "print(f\"Model initialized (optimized for speed and performance):\")\n",
    "print(f\"  Nodes: {num_nodes}\")\n",
    "print(f\"  Node feature dim: {node_feature_dim}\")\n",
    "print(f\"  Hidden dim: {hidden_dim}\")\n",
    "print(f\"  GNN layers: 3 (GraphSAGE), LSTM layers: 2 (Bidirectional)\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing training data...\n",
      "Created 5831 training samples\n",
      "Created 46 batches\n"
     ]
    }
   ],
   "source": [
    "# Prepare training data\n",
    "print(\"Preparing training data...\")\n",
    "\n",
    "# Convert sequences to indices\n",
    "def sequence_to_indices(seq):\n",
    "    return [place_to_idx[place] for place in seq]\n",
    "\n",
    "train_data = []\n",
    "for seq in train_sequences:\n",
    "    indices = sequence_to_indices(seq)\n",
    "    # Use first seq_len-1 as input, last seq_len-1 as target (shifted by 1)\n",
    "    for i in range(len(indices) - 1):\n",
    "        input_seq = indices[:i+1]  # History up to current\n",
    "        target = indices[i+1]  # Next location\n",
    "        train_data.append((input_seq, target))\n",
    "\n",
    "print(f\"Created {len(train_data)} training samples\")\n",
    "\n",
    "# Create data loader with optimized batch size for speed\n",
    "BATCH_SIZE = 128  # Larger batch for faster training\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Collate function to handle variable length sequences\"\"\"\n",
    "    sequences, targets = zip(*batch)\n",
    "    max_len = max(len(seq) for seq in sequences)\n",
    "    \n",
    "    # Pad sequences\n",
    "    padded_sequences = []\n",
    "    for seq in sequences:\n",
    "        padded = seq + [seq[-1]] * (max_len - len(seq))  # Pad with last element\n",
    "        padded_sequences.append(padded)\n",
    "    \n",
    "    return torch.tensor(padded_sequences, dtype=torch.long), torch.tensor(targets, dtype=torch.long)\n",
    "\n",
    "# Simple data loader\n",
    "def create_batches(data, batch_size):\n",
    "    batches = []\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        batch = data[i:i+batch_size]\n",
    "        batches.append(collate_fn(batch))\n",
    "    return batches\n",
    "\n",
    "train_batches = create_batches(train_data, BATCH_SIZE)\n",
    "print(f\"Created {len(train_batches)} batches\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 4957\n",
      "Validation samples: 874\n",
      "\n",
      "Training model for up to 150 epochs (early stopping with patience=25)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150, Train Loss: 3.0443, Val Loss: 2.3137, Val Acc: 0.3581, Best Val Acc: 0.3581, Best Val Loss: 2.3137, LR: 0.000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/150, Train Loss: 2.6461, Val Loss: 2.2869, Val Acc: 0.3547, Best Val Acc: 0.3581, Best Val Loss: 2.3137, LR: 0.000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/150, Train Loss: 2.4817, Val Loss: 2.3196, Val Acc: 0.3890, Best Val Acc: 0.3890, Best Val Loss: 2.3196, LR: 0.000195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/150, Train Loss: 2.3253, Val Loss: 2.2970, Val Acc: 0.3593, Best Val Acc: 0.3890, Best Val Loss: 2.3196, LR: 0.000082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/150, Train Loss: 2.2451, Val Loss: 2.3869, Val Acc: 0.3627, Best Val Acc: 0.3890, Best Val Loss: 2.3196, LR: 0.000054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/150, Train Loss: 2.2004, Val Loss: 2.4478, Val Acc: 0.5378, Best Val Acc: 0.5675, Best Val Loss: 2.4391, LR: 0.000035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/150, Train Loss: 2.1566, Val Loss: 2.4245, Val Acc: 0.6522, Best Val Acc: 0.6751, Best Val Loss: 2.4304, LR: 0.000023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/150, Train Loss: 2.0873, Val Loss: 2.3253, Val Acc: 0.6865, Best Val Acc: 0.6865, Best Val Loss: 2.3253, LR: 0.000015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/150, Train Loss: 2.0609, Val Loss: 2.2704, Val Acc: 0.6796, Best Val Acc: 0.6888, Best Val Loss: 2.2689, LR: 0.000010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90/150, Train Loss: 2.0375, Val Loss: 2.2054, Val Acc: 0.6934, Best Val Acc: 0.6934, Best Val Loss: 2.2344, LR: 0.000010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/150, Train Loss: 1.9990, Val Loss: 2.1731, Val Acc: 0.6911, Best Val Acc: 0.6934, Best Val Loss: 2.2344, LR: 0.000010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 110/150, Train Loss: 1.9594, Val Loss: 2.1507, Val Acc: 0.6911, Best Val Acc: 0.6957, Best Val Loss: 2.1550, LR: 0.000010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/150, Train Loss: 1.9498, Val Loss: 2.1386, Val Acc: 0.6934, Best Val Acc: 0.6957, Best Val Loss: 2.1550, LR: 0.000010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 130/150, Train Loss: 1.9157, Val Loss: 2.1502, Val Acc: 0.6934, Best Val Acc: 0.6957, Best Val Loss: 2.1550, LR: 0.000006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 132\n",
      "Loaded best model from epoch 107 with val loss 2.1550 and val acc 0.6957\n",
      "Training completed!\n",
      "Model saved to /home/root495/Inexture/Location Prediction Update/models/gnn_10users_model_best.pt\n"
     ]
    }
   ],
   "source": [
    "# Training setup with optimized parameters for speed and performance\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Balanced learning rate for better convergence\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.0003, weight_decay=1e-5, betas=(0.9, 0.999))\n",
    "# Better scheduler for learning\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.65, patience=8, verbose=False, min_lr=1e-6)\n",
    "\n",
    "# Move graph data to device\n",
    "x = node_features.to(device)\n",
    "edge_idx = edge_index.to(device)\n",
    "\n",
    "# Create validation split\n",
    "val_split = int(len(train_data) * 0.15)  # 15% for validation\n",
    "val_data = train_data[:val_split]\n",
    "train_data_final = train_data[val_split:]\n",
    "val_batches = create_batches(val_data, BATCH_SIZE)\n",
    "train_batches = create_batches(train_data_final, BATCH_SIZE)\n",
    "\n",
    "print(f\"Training samples: {len(train_data_final)}\")\n",
    "print(f\"Validation samples: {len(val_data)}\")\n",
    "\n",
    "# Training loop optimized for speed\n",
    "NUM_EPOCHS = 150  # Slightly more epochs for better learning\n",
    "best_loss = float('inf')\n",
    "best_val_acc = 0.0  # Track best validation accuracy\n",
    "patience_counter = 0\n",
    "patience = 25  # More patience to allow model to learn better\n",
    "\n",
    "print(f\"\\nTraining model for up to {NUM_EPOCHS} epochs (early stopping with patience={patience})...\")\n",
    "\n",
    "model.train()\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batch_seqs, batch_targets in tqdm(train_batches, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\", leave=False):\n",
    "        batch_seqs = batch_seqs.to(device)\n",
    "        batch_targets = batch_targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(x, edge_idx, batch_seqs)\n",
    "        loss = criterion(outputs, batch_targets)\n",
    "        \n",
    "        # Backward pass with gradient clipping\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Moderate gradient clipping\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "    \n",
    "    avg_loss = total_loss / num_batches if num_batches > 0 else 0\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_batches_count = 0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_seqs, batch_targets in val_batches:\n",
    "            batch_seqs = batch_seqs.to(device)\n",
    "            batch_targets = batch_targets.to(device)\n",
    "            outputs = model(x, edge_idx, batch_seqs)\n",
    "            loss = criterion(outputs, batch_targets)\n",
    "            val_loss += loss.item()\n",
    "            val_batches_count += 1\n",
    "            \n",
    "            # Calculate validation accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_total += batch_targets.size(0)\n",
    "            val_correct += (predicted == batch_targets).sum().item()\n",
    "    \n",
    "    avg_val_loss = val_loss / val_batches_count if val_batches_count > 0 else float('inf')\n",
    "    val_acc = val_correct / val_total if val_total > 0 else 0.0\n",
    "    model.train()\n",
    "    \n",
    "    scheduler.step(avg_val_loss)  # ReduceLROnPlateau uses validation loss\n",
    "    \n",
    "    # Early stopping based on validation accuracy (better metric than loss)\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_loss = avg_val_loss  # Also track best loss\n",
    "        patience_counter = 0\n",
    "        # Save best model\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'epoch': epoch,\n",
    "            'loss': avg_loss,\n",
    "            'val_loss': avg_val_loss,\n",
    "            'val_acc': val_acc\n",
    "        }, MODEL_SAVE_PATH.replace('.pt', '_best.pt'))\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0 or epoch == 0:  # Print every 10 epochs for less output\n",
    "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Train Loss: {avg_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.4f}, Best Val Acc: {best_val_acc:.4f}, Best Val Loss: {best_loss:.4f}, LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    \n",
    "    if patience_counter >= patience:\n",
    "        print(f\"Early stopping at epoch {epoch+1}\")\n",
    "        # Load best model\n",
    "        checkpoint = torch.load(MODEL_SAVE_PATH.replace('.pt', '_best.pt'))\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(f\"Loaded best model from epoch {checkpoint['epoch']+1} with val loss {checkpoint['val_loss']:.4f} and val acc {checkpoint.get('val_acc', 0):.4f}\")\n",
    "        best_val_acc = checkpoint.get('val_acc', 0)\n",
    "        break\n",
    "\n",
    "print(\"Training completed!\")\n",
    "\n",
    "# Save model\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'place_to_idx': place_to_idx,\n",
    "    'idx_to_place': idx_to_place,\n",
    "    'num_nodes': num_nodes,\n",
    "    'node_feature_dim': node_feature_dim,\n",
    "    'hidden_dim': hidden_dim\n",
    "}, MODEL_SAVE_PATH)\n",
    "print(f\"Model saved to {MODEL_SAVE_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8 — Evaluation Setup\n",
    "\n",
    "Prepare test sequences and helper functions for evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using all 30 test sequences for evaluation...\n",
      "Created 1470 test cases from 30 test sequences\n",
      "Loaded coordinates for 2073 places\n",
      "Building transition patterns from training data...\n",
      "Built transition patterns for 286 locations\n",
      "Evaluation setup complete!\n"
     ]
    }
   ],
   "source": [
    "# Use ALL test sequences for evaluation (like HMM does)\n",
    "print(f\"Using all {len(test_sequences)} test sequences for evaluation...\")\n",
    "\n",
    "# Create test cases from all test sequences\n",
    "test_cases = []\n",
    "for test_sequence in test_sequences:\n",
    "    test_indices = sequence_to_indices(test_sequence)\n",
    "    for i in range(1, len(test_indices)):\n",
    "        history = test_indices[:i]\n",
    "        true_next = test_indices[i]\n",
    "        test_cases.append((history, true_next))\n",
    "\n",
    "print(f\"Created {len(test_cases)} test cases from {len(test_sequences)} test sequences\")\n",
    "\n",
    "# Load coordinates for MPD calculation\n",
    "print(f\"Loaded coordinates for {len(place_coords)} places\")\n",
    "\n",
    "# Helper function to get coordinates from place_id\n",
    "def place_id_to_coords(place_id, place_coords, grid_metadata):\n",
    "    \"\"\"Get coordinates from place_id\"\"\"\n",
    "    if place_id is None:\n",
    "        return None, None\n",
    "    \n",
    "    # Try to find in place_coords first\n",
    "    if place_id in place_coords:\n",
    "        return place_coords[place_id]['lat'], place_coords[place_id]['lon']\n",
    "    \n",
    "    # Fallback: calculate from grid if place_id has format \"row_col\"\n",
    "    try:\n",
    "        if \"_\" in str(place_id):\n",
    "            row, col = map(int, str(place_id).split(\"_\"))\n",
    "            lat = grid_metadata['min_lat'] + row * grid_metadata['deg_lat']\n",
    "            lon = grid_metadata['min_lon'] + col * grid_metadata['deg_lon']\n",
    "            return lat, lon\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return None, None\n",
    "\n",
    "# Build transition frequency matrix from training data for pattern-based prediction\n",
    "print(\"Building transition patterns from training data...\")\n",
    "transition_counts = {}\n",
    "for seq in train_sequences:\n",
    "    indices = sequence_to_indices(seq)\n",
    "    for i in range(len(indices) - 1):\n",
    "        current = indices[i]\n",
    "        next_loc = indices[i+1]\n",
    "        if current not in transition_counts:\n",
    "            transition_counts[current] = {}\n",
    "        transition_counts[current][next_loc] = transition_counts[current].get(next_loc, 0) + 1\n",
    "\n",
    "# Convert to probabilities\n",
    "transition_probs = {}\n",
    "for current, next_dict in transition_counts.items():\n",
    "    total = sum(next_dict.values())\n",
    "    transition_probs[current] = {next_loc: count/total for next_loc, count in next_dict.items()}\n",
    "\n",
    "print(f\"Built transition patterns for {len(transition_probs)} locations\")\n",
    "\n",
    "# Improved prediction functions with ensemble and temperature scaling\n",
    "def predict_next_location(model, history, x, edge_idx, device, use_patterns=True, temperature=1.0):\n",
    "    \"\"\"Predict next location using GNN model with pattern-based ensemble (like HMM)\"\"\"\n",
    "    if len(history) == 0:\n",
    "        return None\n",
    "    \n",
    "    # Use GNN model prediction\n",
    "    try:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            seq_tensor = torch.tensor([history], dtype=torch.long).to(device)\n",
    "            output = model(x, edge_idx, seq_tensor)\n",
    "            \n",
    "            # Apply temperature scaling for better calibration\n",
    "            output = output / temperature\n",
    "            \n",
    "            # Combine with pattern-based prediction (like HMM does) - pattern-first approach\n",
    "            if use_patterns and len(history) > 0:\n",
    "                last_obs = history[-1]\n",
    "                if last_obs in transition_probs:\n",
    "                    next_probs = transition_probs[last_obs]\n",
    "                    if next_probs:\n",
    "                        # Create pattern-based distribution\n",
    "                        pattern_logits = torch.zeros_like(output[0])\n",
    "                        max_pattern_prob = 0.0\n",
    "                        for loc_idx, prob in next_probs.items():\n",
    "                            pattern_logits[loc_idx] = prob\n",
    "                            max_pattern_prob = max(max_pattern_prob, prob)\n",
    "                        \n",
    "                        # Adaptive weighting: if pattern is very confident, trust it more\n",
    "                        pattern_confidence = max_pattern_prob\n",
    "                        if pattern_confidence > 0.5:\n",
    "                            # High confidence pattern: 90% pattern, 10% GNN (pattern is very reliable)\n",
    "                            pattern_weight = 0.90\n",
    "                        elif pattern_confidence > 0.3:\n",
    "                            # Medium confidence: 80% pattern, 20% GNN\n",
    "                            pattern_weight = 0.80\n",
    "                        else:\n",
    "                            # Low confidence: 70% pattern, 30% GNN\n",
    "                            pattern_weight = 0.70\n",
    "                        \n",
    "                        # Softmax pattern probabilities with very strong scaling\n",
    "                        pattern_logits = torch.softmax(pattern_logits * 12.0, dim=0)\n",
    "                        gnn_probs = torch.softmax(output, dim=1)\n",
    "                        output = (1 - pattern_weight) * gnn_probs + pattern_weight * pattern_logits.unsqueeze(0)\n",
    "                    else:\n",
    "                        output = torch.softmax(output, dim=1)\n",
    "                else:\n",
    "                    output = torch.softmax(output, dim=1)\n",
    "            else:\n",
    "                output = torch.softmax(output, dim=1)\n",
    "            \n",
    "            pred_idx = output.argmax(dim=1).item()\n",
    "            return pred_idx\n",
    "    except Exception as e:\n",
    "        # Fallback to pattern-based if GNN fails\n",
    "        if len(history) > 0:\n",
    "            last_obs = history[-1]\n",
    "            if last_obs in transition_probs:\n",
    "                next_probs = transition_probs[last_obs]\n",
    "                if next_probs:\n",
    "                    most_likely = max(next_probs.items(), key=lambda x: x[1])[0]\n",
    "                    return int(most_likely)\n",
    "        return None\n",
    "\n",
    "def predict_top_k(model, history, k, x, edge_idx, device, use_patterns=True, temperature=1.0):\n",
    "    \"\"\"Predict top-K next locations using GNN model with pattern-based ensemble\"\"\"\n",
    "    if len(history) == 0:\n",
    "        return []\n",
    "    \n",
    "    # Use GNN model\n",
    "    try:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            seq_tensor = torch.tensor([history], dtype=torch.long).to(device)\n",
    "            output = model(x, edge_idx, seq_tensor)\n",
    "            \n",
    "            # Apply temperature scaling\n",
    "            output = output / temperature\n",
    "            \n",
    "            # Pattern-first approach (like HMM): use pattern as primary, GNN as refinement\n",
    "            if use_patterns and len(history) > 0:\n",
    "                last_obs = history[-1]\n",
    "                if last_obs in transition_probs:\n",
    "                    next_probs = transition_probs[last_obs]\n",
    "                    if next_probs:\n",
    "                        # Create pattern-based distribution\n",
    "                        pattern_logits = torch.zeros_like(output[0])\n",
    "                        max_pattern_prob = 0.0\n",
    "                        for loc_idx, prob in next_probs.items():\n",
    "                            pattern_logits[loc_idx] = prob\n",
    "                            max_pattern_prob = max(max_pattern_prob, prob)\n",
    "                        \n",
    "                        # Very high pattern weight - pattern is primary predictor\n",
    "                        pattern_confidence = max_pattern_prob\n",
    "                        if pattern_confidence > 0.4:\n",
    "                            # Medium-high confidence: 95% pattern, 5% GNN\n",
    "                            pattern_weight = 0.95\n",
    "                        elif pattern_confidence > 0.2:\n",
    "                            # Medium confidence: 90% pattern, 10% GNN\n",
    "                            pattern_weight = 0.90\n",
    "                        else:\n",
    "                            # Low confidence: 85% pattern, 15% GNN\n",
    "                            pattern_weight = 0.85\n",
    "                        \n",
    "                        # Very strong pattern scaling to emphasize pattern predictions\n",
    "                        pattern_logits = torch.softmax(pattern_logits * 15.0, dim=0)\n",
    "                        gnn_probs = torch.softmax(output, dim=1)\n",
    "                        output = (1 - pattern_weight) * gnn_probs + pattern_weight * pattern_logits.unsqueeze(0)\n",
    "                    else:\n",
    "                        output = torch.softmax(output, dim=1)\n",
    "                else:\n",
    "                    output = torch.softmax(output, dim=1)\n",
    "            else:\n",
    "                output = torch.softmax(output, dim=1)\n",
    "            \n",
    "            top_k_values, top_k_indices = torch.topk(output[0], k)\n",
    "            gnn_preds = top_k_indices.cpu().numpy().tolist()\n",
    "            return gnn_preds\n",
    "    except Exception as e:\n",
    "        # Fallback to pattern-based if GNN fails\n",
    "        if len(history) > 0:\n",
    "            last_obs = history[-1]\n",
    "            if last_obs in transition_probs:\n",
    "                next_probs = transition_probs[last_obs]\n",
    "                if next_probs:\n",
    "                    sorted_patterns = sorted(next_probs.items(), key=lambda x: x[1], reverse=True)\n",
    "                    return [int(loc) for loc, _ in sorted_patterns[:k]]\n",
    "        return []\n",
    "\n",
    "print(\"Evaluation setup complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 9 — Metric 1: Accuracy\n",
    "\n",
    "**Definition**: How many times your model predicted the correct next location.\n",
    "\n",
    "Accuracy: Part of correct prediction to total prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Accuracy...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Making predictions: 100%|██████████| 1470/1470 [00:08<00:00, 165.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Debug - First 5 predictions:\n",
      "  ✗ Pred: 219 (296_2075) | True: 213 (295_2076)\n",
      "  ✗ Pred: 220 (296_2076) | True: 214 (295_2077)\n",
      "  ✓ Pred: 213 (295_2076) | True: 213 (295_2076)\n",
      "  ✓ Pred: 220 (296_2076) | True: 220 (296_2076)\n",
      "  ✓ Pred: 219 (296_2075) | True: 219 (296_2075)\n",
      "\n",
      "============================================================\n",
      "METRIC 1: ACCURACY\n",
      "============================================================\n",
      "Correct predictions: 742\n",
      "Total predictions: 1470\n",
      "Accuracy: 0.504761904762\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate Accuracy with improved prediction (using pattern ensemble like HMM)\n",
    "print(\"Calculating Accuracy...\")\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "# Use temperature scaling and pattern ensemble (optimized for best results)\n",
    "TEMPERATURE = 1.0  # Standard temperature for pattern-first approach\n",
    "\n",
    "for history, true_next in tqdm(test_cases, desc=\"Making predictions\"):\n",
    "    pred = predict_next_location(model, history, x, edge_idx, device, use_patterns=True, temperature=TEMPERATURE)\n",
    "    if pred is not None:\n",
    "        predictions.append(pred)\n",
    "        true_labels.append(true_next)\n",
    "\n",
    "# Calculate accuracy\n",
    "if len(predictions) == 0:\n",
    "    print(\"ERROR: No predictions were made!\")\n",
    "    accuracy = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "else:\n",
    "    correct = sum(1 for p, t in zip(predictions, true_labels) if p == t)\n",
    "    total = len(predictions)\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    \n",
    "    # Debug: Show first few predictions vs true\n",
    "    print(f\"\\nDebug - First 5 predictions:\")\n",
    "    for i in range(min(5, len(predictions))):\n",
    "        pred_place = idx_to_place.get(predictions[i], \"Unknown\")\n",
    "        true_place = idx_to_place.get(true_labels[i], \"Unknown\")\n",
    "        match = \"✓\" if predictions[i] == true_labels[i] else \"✗\"\n",
    "        print(f\"  {match} Pred: {predictions[i]} ({pred_place[:20]}) | True: {true_labels[i]} ({true_place[:20]})\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"METRIC 1: ACCURACY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Correct predictions: {correct}\")\n",
    "print(f\"Total predictions: {total}\")\n",
    "print(f\"Accuracy: {accuracy:.12f}\")\n",
    "print(f\"{'='*60}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 10 — Metric 2: Precision & Recall\n",
    "\n",
    "**Definition**: \n",
    "- **Precision**: How many predicted locations were actually correct, weighted by class frequency\n",
    "- **Recall**: Out of all true next locations, how many you successfully predicted, weighted by class frequency\n",
    "\n",
    "Measuring how trustworthy the model is with visited and predicted locations using weighted averages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Precision & Recall (Weighted)...\n",
      "\n",
      "============================================================\n",
      "METRIC 2: PRECISION & RECALL\n",
      "============================================================\n",
      "Precision: 0.438886055101\n",
      "Recall: 0.504761904762\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Calculate Precision & Recall (Weighted)\n",
    "print(\"Calculating Precision & Recall (Weighted)...\")\n",
    "\n",
    "if len(predictions) > 0:\n",
    "    precision_weighted = precision_score(true_labels, predictions, average='weighted', zero_division=0)\n",
    "    recall_weighted = recall_score(true_labels, predictions, average='weighted', zero_division=0)\n",
    "else:\n",
    "    precision_weighted = recall_weighted = 0\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"METRIC 2: PRECISION & RECALL\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Precision: {precision_weighted:.12f}\")\n",
    "print(f\"Recall: {recall_weighted:.12f}\")\n",
    "print(f\"{'='*60}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 11 — Metric 3: Top-K Accuracy\n",
    "\n",
    "**Definition**: The true next location is considered correct if it appears in the top K predicted locations.\n",
    "\n",
    "Top-K Accuracy: If the true next position is included in the top-K predictions (K=1, 3, 5).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Top-K Accuracy...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Top-1: 100%|██████████| 1470/1470 [00:07<00:00, 189.11it/s]\n",
      "Top-3: 100%|██████████| 1470/1470 [00:07<00:00, 193.67it/s]\n",
      "Top-5: 100%|██████████| 1470/1470 [00:07<00:00, 202.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "METRIC 3: TOP-K ACCURACY\n",
      "============================================================\n",
      "Top-1 Accuracy: 0.504761904762\n",
      "Top-3 Accuracy: 0.691836734694\n",
      "Top-5 Accuracy: 0.787074829932\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate Top-K Accuracy\n",
    "print(\"Calculating Top-K Accuracy...\")\n",
    "\n",
    "k_values = [1, 3, 5]\n",
    "top_k_results = {}\n",
    "\n",
    "for k in k_values:\n",
    "    correct_k = 0\n",
    "    total_k = 0\n",
    "    \n",
    "    for history, true_next in tqdm(test_cases, desc=f\"Top-{k}\"):\n",
    "        top_k_preds = predict_top_k(model, history, k, x, edge_idx, device, use_patterns=True, temperature=TEMPERATURE)\n",
    "        if len(top_k_preds) > 0:\n",
    "            total_k += 1\n",
    "            if true_next in top_k_preds:\n",
    "                correct_k += 1\n",
    "    \n",
    "    top_k_accuracy = correct_k / total_k if total_k > 0 else 0\n",
    "    top_k_results[k] = {\n",
    "        'correct': correct_k,\n",
    "        'total': total_k,\n",
    "        'accuracy': top_k_accuracy\n",
    "    }\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"METRIC 3: TOP-K ACCURACY\")\n",
    "print(f\"{'='*60}\")\n",
    "for k in k_values:\n",
    "    result = top_k_results[k]\n",
    "    print(f\"Top-{k} Accuracy: {result['accuracy']:.12f}\")\n",
    "print(f\"{'='*60}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 12 — Metric 4: Mean Prediction Distance (MPD)\n",
    "\n",
    "**Definition**: Average Haversine distance (in meters) between actual next location and predicted next location.\n",
    "\n",
    "MPD Distance: Mean Prediction Distance — Mean actual distance visited from predicted location of next visit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Mean Prediction Distance (MPD)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating distances: 100%|██████████| 1470/1470 [00:08<00:00, 178.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: 73 distance calculations failed or were filtered\n",
      "\n",
      "============================================================\n",
      "METRIC 4: MEAN PREDICTION DISTANCE (MPD)\n",
      "============================================================\n",
      "MPD Distance: 32168.614285539221 meters\n",
      "Valid distance calculations: 1397/1470\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate Mean Prediction Distance (MPD)\n",
    "print(\"Calculating Mean Prediction Distance (MPD)...\")\n",
    "\n",
    "distances = []\n",
    "failed_conversions = 0\n",
    "\n",
    "for history, true_next in tqdm(test_cases, desc=\"Calculating distances\"):\n",
    "    pred = predict_next_location(model, history, x, edge_idx, device, use_patterns=True, temperature=TEMPERATURE)\n",
    "    \n",
    "    if pred is not None:\n",
    "        # Convert indices back to place_ids\n",
    "        pred_place_id = idx_to_place.get(pred)\n",
    "        true_place_id = idx_to_place.get(true_next)\n",
    "        \n",
    "        if pred_place_id and true_place_id:\n",
    "            # Get coordinates\n",
    "            pred_lat, pred_lon = place_id_to_coords(pred_place_id, place_coords, grid_metadata)\n",
    "            true_lat, true_lon = place_id_to_coords(true_place_id, place_coords, grid_metadata)\n",
    "            \n",
    "            if pred_lat is not None and true_lat is not None:\n",
    "                # Calculate haversine distance\n",
    "                try:\n",
    "                    distance_m = haversine((pred_lat, pred_lon), (true_lat, true_lon)) * 1000\n",
    "                    # Filter out unrealistic distances (likely coordinate errors)\n",
    "                    if distance_m < 1000000:  # Less than 1000 km\n",
    "                        distances.append(distance_m)\n",
    "                    else:\n",
    "                        failed_conversions += 1\n",
    "                except:\n",
    "                    failed_conversions += 1\n",
    "            else:\n",
    "                failed_conversions += 1\n",
    "        else:\n",
    "            failed_conversions += 1\n",
    "    else:\n",
    "        failed_conversions += 1\n",
    "\n",
    "if failed_conversions > 0:\n",
    "    print(f\"Warning: {failed_conversions} distance calculations failed or were filtered\")\n",
    "\n",
    "mpd = np.mean(distances) if len(distances) > 0 else 0\n",
    "mpd_median = np.median(distances) if len(distances) > 0 else 0\n",
    "mpd_std = np.std(distances) if len(distances) > 0 else 0\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"METRIC 4: MEAN PREDICTION DISTANCE (MPD)\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"MPD Distance: {mpd:.12f} meters\")\n",
    "print(f\"Valid distance calculations: {len(distances)}/{len(test_cases)}\")\n",
    "print(f\"{'='*60}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 13 — Results Summary\n",
    "\n",
    "Summary of all evaluation metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "EVALUATION RESULTS SUMMARY\n",
      "============================================================\n",
      "\n",
      "Number of users: 10\n",
      "Users: ['000', '001', '005', '006', '009', '011', '014', '016', '019', '025']\n",
      "Total original places: 1752364\n",
      "After duplicate removal: 4087\n",
      "Training sequences: 119\n",
      "Test sequences: 30\n",
      "\n",
      "1. ACCURACY\n",
      "   Accuracy: 0.504761904762\n",
      "\n",
      "2. PRECISION & RECALL\n",
      "   Precision: 0.438886055101\n",
      "   Recall: 0.504761904762\n",
      "\n",
      "3. TOP-K ACCURACY\n",
      "   Top-1 Accuracy: 0.504761904762\n",
      "   Top-3 Accuracy: 0.691836734694\n",
      "   Top-5 Accuracy: 0.787074829932\n",
      "\n",
      "4. MEAN PREDICTION DISTANCE (MPD)\n",
      "   MPD Distance: 32168.614285539221 meters\n",
      "\n",
      "============================================================\n",
      "\n",
      "Results saved to /home/root495/Inexture/Location Prediction Update/results/gnn_10users_results.json\n",
      "\n",
      "Results Table:\n",
      "        Metric              Value\n",
      "      Accuracy     0.504761904762\n",
      "     Precision     0.438886055101\n",
      "        Recall     0.504761904762\n",
      "Top-1 Accuracy     0.504761904762\n",
      "Top-3 Accuracy     0.691836734694\n",
      "Top-5 Accuracy     0.787074829932\n",
      "  MPD Distance 32168.614285539221\n"
     ]
    }
   ],
   "source": [
    "# Compile all results\n",
    "results = {\n",
    "    'num_users': len(user_sequences),\n",
    "    'selected_users': list(user_sequences.keys()),\n",
    "    'preprocessing': {\n",
    "        'total_original_places': total_original,\n",
    "        'total_after_duplicate_removal': total_processed,\n",
    "        'total_duplicates_removed': total_original - total_processed,\n",
    "        'sequence_length': SEQUENCE_LENGTH,\n",
    "        'total_sequences': len(all_sequences),\n",
    "        'training_sequences': len(train_sequences),\n",
    "        'test_sequences': len(test_sequences)\n",
    "    },\n",
    "    'model': {\n",
    "        'num_nodes': num_nodes,\n",
    "        'node_feature_dim': node_feature_dim,\n",
    "        'hidden_dim': hidden_dim,\n",
    "        'architecture': 'GCN + LSTM'\n",
    "    },\n",
    "    'accuracy': {\n",
    "        'value': accuracy,\n",
    "        'correct': correct,\n",
    "        'total': total\n",
    "    },\n",
    "    'precision_recall': {\n",
    "        'precision': float(precision_weighted),\n",
    "        'recall': float(recall_weighted)\n",
    "    },\n",
    "    'top_k_accuracy': {\n",
    "        f'top_{k}_accuracy': float(top_k_results[k]['accuracy']) for k in k_values\n",
    "    },\n",
    "    'mpd_distance': {\n",
    "        'mpd_distance_meters': float(mpd),\n",
    "        'valid_calculations': len(distances)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Display summary\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"EVALUATION RESULTS SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nNumber of users: {len(user_sequences)}\")\n",
    "print(f\"Users: {list(user_sequences.keys())}\")\n",
    "print(f\"Total original places: {total_original}\")\n",
    "print(f\"After duplicate removal: {total_processed}\")\n",
    "print(f\"Training sequences: {len(train_sequences)}\")\n",
    "print(f\"Test sequences: {len(test_sequences)}\")\n",
    "\n",
    "print(f\"\\n1. ACCURACY\")\n",
    "print(f\"   Accuracy: {accuracy:.12f}\")\n",
    "\n",
    "print(f\"\\n2. PRECISION & RECALL\")\n",
    "print(f\"   Precision: {precision_weighted:.12f}\")\n",
    "print(f\"   Recall: {recall_weighted:.12f}\")\n",
    "\n",
    "print(f\"\\n3. TOP-K ACCURACY\")\n",
    "for k in k_values:\n",
    "    acc = top_k_results[k]['accuracy']\n",
    "    print(f\"   Top-{k} Accuracy: {acc:.12f}\")\n",
    "\n",
    "print(f\"\\n4. MEAN PREDICTION DISTANCE (MPD)\")\n",
    "print(f\"   MPD Distance: {mpd:.12f} meters\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "\n",
    "# Save results\n",
    "with open(RESULTS_SAVE_PATH, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"\\nResults saved to {RESULTS_SAVE_PATH}\")\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame({\n",
    "    'Metric': [\n",
    "        'Accuracy',\n",
    "        'Precision',\n",
    "        'Recall',\n",
    "        'Top-1 Accuracy',\n",
    "        'Top-3 Accuracy',\n",
    "        'Top-5 Accuracy',\n",
    "        'MPD Distance'\n",
    "    ],\n",
    "    'Value': [\n",
    "        f\"{accuracy:.12f}\",\n",
    "        f\"{precision_weighted:.12f}\",\n",
    "        f\"{recall_weighted:.12f}\",\n",
    "        f\"{top_k_results[1]['accuracy']:.12f}\",\n",
    "        f\"{top_k_results[3]['accuracy']:.12f}\",\n",
    "        f\"{top_k_results[5]['accuracy']:.12f}\",\n",
    "        f\"{mpd:.12f}\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\nResults Table:\")\n",
    "print(results_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
