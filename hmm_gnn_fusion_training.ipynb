{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HMM-GNN Fusion Model Training\n",
    "\n",
    "This notebook:\n",
    "- Loads trained HMM and GNN models\n",
    "- Extracts HMM state probabilities and GNN node embeddings\n",
    "- Fuses features and trains a hybrid MLP classifier\n",
    "- Evaluates Fusion model using Accuracy, Precision & Recall, Top-K Accuracy, and MPD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1 — Imports & Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from haversine import haversine\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import GCNConv, SAGEConv, GATConv\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "# Paths\n",
    "BASE_PATH = \"/home/root495/Inexture/Location Prediction Update\"\n",
    "PROCESSED_PATH = BASE_PATH + \"/data/processed/\"\n",
    "SEQUENCES_FILE = PROCESSED_PATH + \"place_sequences.json\"\n",
    "GRAPH_EDGES_FILE = PROCESSED_PATH + \"graph_edges.csv\"\n",
    "NODE_FEATURES_FILE = PROCESSED_PATH + \"node_features.csv\"\n",
    "GRID_METADATA_FILE = PROCESSED_PATH + \"grid_metadata.json\"\n",
    "CLEANED_WITH_PLACES_FILE = PROCESSED_PATH + \"cleaned_with_places.csv\"\n",
    "OUTPUT_PATH = BASE_PATH + \"/notebooks/\"\n",
    "MODELS_PATH = BASE_PATH + \"/models/\"\n",
    "RESULTS_PATH = BASE_PATH + \"/results/\"\n",
    "\n",
    "# Model paths\n",
    "HMM_MODEL_PATH = MODELS_PATH + \"hmm_10users_model.pkl\"\n",
    "GNN_MODEL_PATH = MODELS_PATH + \"gnn_10users_model_best.pt\"\n",
    "FUSION_MODEL_PATH = MODELS_PATH + \"fusion_model_best.pt\"\n",
    "RESULTS_SAVE_PATH = RESULTS_PATH + \"fusion_results.json\"\n",
    "\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "os.makedirs(MODELS_PATH, exist_ok=True)\n",
    "os.makedirs(RESULTS_PATH, exist_ok=True)\n",
    "\n",
    "# Specific users to use (same as GNN notebook)\n",
    "SELECTED_USERS = ['000', '001', '005', '006', '009', '011', '014', '016', '019', '025']\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(\"Libraries imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2 — Load Trained Models\n",
    "\n",
    "Load the pre-trained HMM and GNN models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading HMM model...\n",
      "HMM model loaded:\n",
      "  Number of hidden states: 50\n",
      "  Number of observable states: 303\n",
      "  LabelEncoder classes: 303\n",
      "\n",
      "Loading GNN model...\n",
      "GNN checkpoint loaded:\n",
      "  Keys: ['model_state_dict', 'place_to_idx', 'idx_to_place', 'num_nodes', 'node_feature_dim', 'hidden_dim']\n",
      "  Model state dict found\n",
      "\n",
      "Models loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load HMM model\n",
    "print(\"Loading HMM model...\")\n",
    "with open(HMM_MODEL_PATH, 'rb') as f:\n",
    "    hmm_model = pickle.load(f)\n",
    "    hmm_le = pickle.load(f)  # LabelEncoder\n",
    "    hmm_encoded_to_placeid = pickle.load(f)  # Mapping\n",
    "\n",
    "print(f\"HMM model loaded:\")\n",
    "print(f\"  Number of hidden states: {hmm_model.n_components}\")\n",
    "print(f\"  Number of observable states: {hmm_model.n_features}\")\n",
    "print(f\"  LabelEncoder classes: {len(hmm_le.classes_)}\")\n",
    "\n",
    "# Load GNN model\n",
    "print(\"\\nLoading GNN model...\")\n",
    "checkpoint = torch.load(GNN_MODEL_PATH, map_location=device)\n",
    "\n",
    "# We need to reconstruct the GNN model architecture\n",
    "# First, we'll need to know the model parameters - we'll get them from the checkpoint or rebuild\n",
    "# For now, we'll load it later after we know the graph structure\n",
    "print(f\"GNN checkpoint loaded:\")\n",
    "print(f\"  Keys: {list(checkpoint.keys())}\")\n",
    "if 'model_state_dict' in checkpoint:\n",
    "    print(f\"  Model state dict found\")\n",
    "    gnn_checkpoint = checkpoint\n",
    "else:\n",
    "    gnn_checkpoint = checkpoint\n",
    "\n",
    "print(\"\\nModels loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3 — Load & Prepare Data\n",
    "\n",
    "Load sequences and apply the same preprocessing as the GNN notebook (remove consecutive duplicates, create sequences of length 50).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading place sequences...\n",
      "Total users available: 54\n",
      "  User 000: 173817 places\n",
      "  User 001: 108561 places\n",
      "  User 005: 108967 places\n",
      "  User 006: 31809 places\n",
      "  User 009: 84573 places\n",
      "  User 011: 90770 places\n",
      "  User 014: 388051 places\n",
      "  User 016: 89208 places\n",
      "  User 019: 47792 places\n",
      "  User 025: 628816 places\n",
      "\n",
      "Selected 10 users: ['000', '001', '005', '006', '009', '011', '014', '016', '019', '025']\n",
      "Total places across all users: 1752364\n",
      "\n",
      "Removing consecutive duplicates...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing users: 100%|██████████| 10/10 [00:00<00:00, 32.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total places after duplicate removal: 4087\n",
      "\n",
      "Creating sequences of length 50...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating sequences: 100%|██████████| 10/10 [00:00<00:00, 22239.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sequences created: 149\n",
      "Training sequences: 119\n",
      "Test sequences: 30\n",
      "\n",
      "Building graph structure...\n",
      "Total unique places (nodes): 303\n",
      "Total edges: 690\n",
      "\n",
      "Preparing node features...\n",
      "\n",
      "Extracting HMM features for each node...\n",
      "HMM node features shape: (303, 50)\n",
      "Enhanced node features shape: torch.Size([303, 56])\n",
      "Node features: [visit_frequency, lat, lon, in_degree, out_degree, max_transition_prob, HMM_state_0, ..., HMM_state_49]\n",
      "Data preparation complete!\n"
     ]
    }
   ],
   "source": [
    "# Load place sequences\n",
    "print(\"Loading place sequences...\")\n",
    "with open(SEQUENCES_FILE, 'r') as f:\n",
    "    sequences_dict = json.load(f)\n",
    "\n",
    "print(f\"Total users available: {len(sequences_dict)}\")\n",
    "\n",
    "# Load sequences for specific users\n",
    "user_sequences = {}\n",
    "total_places = 0\n",
    "for user_id in SELECTED_USERS:\n",
    "    if user_id in sequences_dict:\n",
    "        seq = sequences_dict[user_id]\n",
    "        user_sequences[user_id] = seq\n",
    "        total_places += len(seq)\n",
    "        print(f\"  User {user_id}: {len(seq)} places\")\n",
    "    else:\n",
    "        print(f\"  Warning: User {user_id} not found in sequences!\")\n",
    "\n",
    "print(f\"\\nSelected {len(user_sequences)} users: {list(user_sequences.keys())}\")\n",
    "print(f\"Total places across all users: {total_places}\")\n",
    "\n",
    "# Remove consecutive duplicates (same as GNN notebook)\n",
    "def remove_consecutive_duplicates(sequence):\n",
    "    \"\"\"Remove consecutive duplicates from sequence.\"\"\"\n",
    "    if len(sequence) == 0:\n",
    "        return sequence\n",
    "    processed = [sequence[0]]\n",
    "    for i in range(1, len(sequence)):\n",
    "        if sequence[i] != sequence[i-1]:\n",
    "            processed.append(sequence[i])\n",
    "    return processed\n",
    "\n",
    "# Apply consecutive duplicate removal\n",
    "processed_sequences = {}\n",
    "print(\"\\nRemoving consecutive duplicates...\")\n",
    "for user_id in tqdm(list(user_sequences.keys()), desc=\"Processing users\"):\n",
    "    original_seq = user_sequences[user_id]\n",
    "    processed_seq = remove_consecutive_duplicates(original_seq)\n",
    "    processed_sequences[user_id] = processed_seq\n",
    "\n",
    "total_processed = sum(len(seq) for seq in processed_sequences.values())\n",
    "print(f\"Total places after duplicate removal: {total_processed}\")\n",
    "\n",
    "# Create sequences of fixed length 50 (same as GNN notebook)\n",
    "SEQUENCE_LENGTH = 50\n",
    "step_size = 25  # 50% overlap\n",
    "\n",
    "all_sequences = []\n",
    "print(\"\\nCreating sequences of length 50...\")\n",
    "for user_id in tqdm(list(processed_sequences.keys()), desc=\"Creating sequences\"):\n",
    "    processed_seq = processed_sequences[user_id]\n",
    "    for i in range(0, len(processed_seq) - SEQUENCE_LENGTH + 1, step_size):\n",
    "        chunk = processed_seq[i:i+SEQUENCE_LENGTH]\n",
    "        if len(chunk) == SEQUENCE_LENGTH:\n",
    "            all_sequences.append(chunk)\n",
    "\n",
    "print(f\"Total sequences created: {len(all_sequences)}\")\n",
    "\n",
    "# Split into train/test (80/20)\n",
    "split_idx = int(len(all_sequences) * 0.8)\n",
    "train_sequences = all_sequences[:split_idx]\n",
    "test_sequences = all_sequences[split_idx:]\n",
    "\n",
    "print(f\"Training sequences: {len(train_sequences)}\")\n",
    "print(f\"Test sequences: {len(test_sequences)}\")\n",
    "\n",
    "# Build graph structure (same as GNN notebook)\n",
    "print(\"\\nBuilding graph structure...\")\n",
    "all_places = set()\n",
    "for seq in processed_sequences.values():\n",
    "    all_places.update(seq)\n",
    "\n",
    "place_to_idx = {place: idx for idx, place in enumerate(sorted(all_places))}\n",
    "idx_to_place = {idx: place for place, idx in place_to_idx.items()}\n",
    "num_nodes = len(place_to_idx)\n",
    "\n",
    "print(f\"Total unique places (nodes): {num_nodes}\")\n",
    "\n",
    "# Build edge list\n",
    "edge_index = []\n",
    "edge_weights = []\n",
    "transition_counts = {}\n",
    "\n",
    "for seq in processed_sequences.values():\n",
    "    for i in range(len(seq) - 1):\n",
    "        source = seq[i]\n",
    "        target = seq[i+1]\n",
    "        source_idx = place_to_idx[source]\n",
    "        target_idx = place_to_idx[target]\n",
    "        if (source_idx, target_idx) not in transition_counts:\n",
    "            transition_counts[(source_idx, target_idx)] = 0\n",
    "        transition_counts[(source_idx, target_idx)] += 1\n",
    "\n",
    "for (source_idx, target_idx), count in transition_counts.items():\n",
    "    edge_index.append([source_idx, target_idx])\n",
    "    edge_weights.append(count)\n",
    "\n",
    "edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "edge_weights = torch.tensor(edge_weights, dtype=torch.float)\n",
    "\n",
    "print(f\"Total edges: {len(edge_weights)}\")\n",
    "\n",
    "# Prepare node features (same as GNN notebook)\n",
    "print(\"\\nPreparing node features...\")\n",
    "df_places = pd.read_csv(CLEANED_WITH_PLACES_FILE)\n",
    "place_coords = df_places.groupby('place_id')[['lat', 'lon']].first().to_dict('index')\n",
    "\n",
    "with open(GRID_METADATA_FILE, 'r') as f:\n",
    "    grid_metadata = json.load(f)\n",
    "\n",
    "# Calculate visit frequency\n",
    "place_visit_counts = {}\n",
    "for seq in train_sequences + test_sequences:\n",
    "    for place in seq:\n",
    "        place_visit_counts[place] = place_visit_counts.get(place, 0) + 1\n",
    "\n",
    "# Calculate in-degree and out-degree for each node (matching training notebook)\n",
    "in_degree = {idx: 0 for idx in range(num_nodes)}\n",
    "out_degree = {idx: 0 for idx in range(num_nodes)}\n",
    "transition_freq = {idx: {} for idx in range(num_nodes)}\n",
    "\n",
    "for seq in train_sequences + test_sequences:\n",
    "    indices = [place_to_idx[place] for place in seq if place in place_to_idx]\n",
    "    for i in range(len(indices) - 1):\n",
    "        source_idx = indices[i]\n",
    "        target_idx = indices[i+1]\n",
    "        out_degree[source_idx] += 1\n",
    "        in_degree[target_idx] += 1\n",
    "        if target_idx not in transition_freq[source_idx]:\n",
    "            transition_freq[source_idx][target_idx] = 0\n",
    "        transition_freq[source_idx][target_idx] += 1\n",
    "\n",
    "# Calculate transition probability (max transition prob from this node)\n",
    "max_transition_prob = {}\n",
    "for source_idx in range(num_nodes):\n",
    "    if len(transition_freq[source_idx]) > 0:\n",
    "        total = sum(transition_freq[source_idx].values())\n",
    "        max_prob = max(transition_freq[source_idx].values()) / total if total > 0 else 0.0\n",
    "        max_transition_prob[source_idx] = max_prob\n",
    "    else:\n",
    "        max_transition_prob[source_idx] = 0.0\n",
    "\n",
    "# Collect coordinates for normalization\n",
    "all_lats = []\n",
    "all_lons = []\n",
    "for idx in range(num_nodes):\n",
    "    place_id = idx_to_place[idx]\n",
    "    if place_id in place_coords:\n",
    "        all_lats.append(place_coords[place_id]['lat'])\n",
    "        all_lons.append(place_coords[place_id]['lon'])\n",
    "    else:\n",
    "        try:\n",
    "            if \"_\" in str(place_id):\n",
    "                row, col = map(int, str(place_id).split(\"_\"))\n",
    "                lat = grid_metadata['min_lat'] + row * grid_metadata['deg_lat']\n",
    "                lon = grid_metadata['min_lon'] + col * grid_metadata['deg_lon']\n",
    "                all_lats.append(lat)\n",
    "                all_lons.append(lon)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "min_lat = min(all_lats) if all_lats else grid_metadata['min_lat']\n",
    "max_lat = max(all_lats) if all_lats else grid_metadata['min_lat'] + 100 * grid_metadata['deg_lat']\n",
    "min_lon = min(all_lons) if all_lons else grid_metadata['min_lon']\n",
    "max_lon = max(all_lons) if all_lons else grid_metadata['min_lon'] + 100 * grid_metadata['deg_lon']\n",
    "\n",
    "# Normalize degrees\n",
    "max_in_degree = max(in_degree.values()) if in_degree.values() else 1\n",
    "max_out_degree = max(out_degree.values()) if out_degree.values() else 1\n",
    "\n",
    "# Build node features with all 6 features (matching training notebook)\n",
    "node_features = []\n",
    "for idx in range(num_nodes):\n",
    "    place_id = idx_to_place[idx]\n",
    "    \n",
    "    # Feature 1: Visit frequency (normalized)\n",
    "    visit_freq = place_visit_counts.get(place_id, 0)\n",
    "    max_visits = max(place_visit_counts.values()) if place_visit_counts else 1\n",
    "    visit_freq_norm = visit_freq / max_visits if max_visits > 0 else 0.0\n",
    "    \n",
    "    # Feature 2 & 3: Coordinates\n",
    "    if place_id in place_coords:\n",
    "        lat = place_coords[place_id]['lat']\n",
    "        lon = place_coords[place_id]['lon']\n",
    "    else:\n",
    "        try:\n",
    "            if \"_\" in str(place_id):\n",
    "                row, col = map(int, str(place_id).split(\"_\"))\n",
    "                lat = grid_metadata['min_lat'] + row * grid_metadata['deg_lat']\n",
    "                lon = grid_metadata['min_lon'] + col * grid_metadata['deg_lon']\n",
    "            else:\n",
    "                lat, lon = min_lat, min_lon\n",
    "        except:\n",
    "            lat, lon = min_lat, min_lon\n",
    "    \n",
    "    # Normalize coordinates\n",
    "    lat_norm = (lat - min_lat) / (max_lat - min_lat + 1e-8)\n",
    "    lon_norm = (lon - min_lon) / (max_lon - min_lon + 1e-8)\n",
    "    \n",
    "    # Feature 4: In-degree (normalized)\n",
    "    in_deg_norm = in_degree[idx] / max_in_degree if max_in_degree > 0 else 0.0\n",
    "    \n",
    "    # Feature 5: Out-degree (normalized)\n",
    "    out_deg_norm = out_degree[idx] / max_out_degree if max_out_degree > 0 else 0.0\n",
    "    \n",
    "    # Feature 6: Max transition probability\n",
    "    max_trans_prob = max_transition_prob[idx]\n",
    "    \n",
    "    node_features.append([visit_freq_norm, lat_norm, lon_norm, in_deg_norm, out_deg_norm, max_trans_prob])\n",
    "\n",
    "# Extract HMM features for each node\n",
    "print(\"\\nExtracting HMM features for each node...\")\n",
    "hmm_node_features = []\n",
    "\n",
    "# Get HMM emission probabilities for each location\n",
    "# For each location, get the emission probabilities (probability of each hidden state when this location is observed)\n",
    "for idx in range(num_nodes):\n",
    "    place_id = idx_to_place[idx]\n",
    "    \n",
    "    # Try to get HMM emission probabilities for this location\n",
    "    try:\n",
    "        # Encode the place_id using HMM's LabelEncoder\n",
    "        if place_id in hmm_le.classes_:\n",
    "            encoded_place = hmm_le.transform([place_id])[0]\n",
    "            # Get emission probabilities: P(hidden_state | observation=this_place)\n",
    "            # emissionprob_ shape: [n_hidden_states, n_observable_states]\n",
    "            emission_probs = hmm_model.emissionprob_[:, encoded_place]  # [n_hidden_states]\n",
    "            hmm_node_features.append(emission_probs)\n",
    "        else:\n",
    "            # If place_id not in HMM encoder, use uniform distribution\n",
    "            hmm_node_features.append(np.ones(hmm_model.n_components) / hmm_model.n_components)\n",
    "    except Exception as e:\n",
    "        # Fallback: use uniform distribution\n",
    "        hmm_node_features.append(np.ones(hmm_model.n_components) / hmm_model.n_components)\n",
    "\n",
    "hmm_node_features = np.array(hmm_node_features)  # [num_nodes, n_hidden_states]\n",
    "print(f\"HMM node features shape: {hmm_node_features.shape}\")\n",
    "\n",
    "# Concatenate original features with HMM features\n",
    "node_features_list = []\n",
    "for idx in range(num_nodes):\n",
    "    original_features = node_features[idx]  # 6 features (list)\n",
    "    hmm_features = hmm_node_features[idx].tolist()  # n_components features (50)\n",
    "    node_features_list.append(original_features + hmm_features)\n",
    "\n",
    "node_features = torch.tensor(node_features_list, dtype=torch.float)\n",
    "print(f\"Enhanced node features shape: {node_features.shape}\")\n",
    "print(f\"Node features: [visit_frequency, lat, lon, in_degree, out_degree, max_transition_prob, HMM_state_0, ..., HMM_state_{hmm_model.n_components-1}]\")\n",
    "print(\"Data preparation complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting HMM state probabilities...\n",
      "Processing training sequences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting HMM features (train): 100%|██████████| 119/119 [00:03<00:00, 35.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test sequences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting HMM features (test): 100%|██████████| 30/30 [00:01<00:00, 28.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HMM features extracted:\n",
      "  Training: (119, 50)\n",
      "  Test: (30, 50)\n",
      "  Feature dimension (hidden states): 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract HMM state probabilities for sequences\n",
    "print(\"Extracting HMM state probabilities...\")\n",
    "\n",
    "def extract_hmm_features(sequence, hmm_model, hmm_le):\n",
    "    \"\"\"Extract HMM state probabilities for a sequence.\"\"\"\n",
    "    try:\n",
    "        # Encode sequence using HMM's LabelEncoder\n",
    "        # First, check if place_ids need to be converted\n",
    "        encoded_seq = []\n",
    "        for place_id in sequence:\n",
    "            # Try to encode directly\n",
    "            try:\n",
    "                encoded = hmm_le.transform([place_id])[0]\n",
    "                encoded_seq.append(encoded)\n",
    "            except ValueError:\n",
    "                # If not in encoder, skip or use a default\n",
    "                continue\n",
    "        \n",
    "        if len(encoded_seq) < 2:\n",
    "            return None\n",
    "        \n",
    "        # Convert to numpy array and reshape for HMM\n",
    "        X = np.array(encoded_seq, dtype=np.int64).reshape(-1, 1)\n",
    "        \n",
    "        # Get state probabilities using predict_proba\n",
    "        # This returns probabilities for each hidden state at each time step\n",
    "        state_probs = hmm_model.predict_proba(X)\n",
    "        \n",
    "        # Extract last state probability vector (temporal context)\n",
    "        if len(state_probs) > 0:\n",
    "            last_state_probs = state_probs[-1]  # Shape: [n_hidden_states]\n",
    "            return last_state_probs\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting HMM features: {e}\")\n",
    "        return None\n",
    "\n",
    "# Extract HMM features for all sequences\n",
    "hmm_features_train = []\n",
    "hmm_features_test = []\n",
    "\n",
    "print(\"Processing training sequences...\")\n",
    "for seq in tqdm(train_sequences, desc=\"Extracting HMM features (train)\"):\n",
    "    hmm_feat = extract_hmm_features(seq, hmm_model, hmm_le)\n",
    "    if hmm_feat is not None:\n",
    "        hmm_features_train.append(hmm_feat)\n",
    "    else:\n",
    "        # Create zero vector if extraction fails\n",
    "        hmm_features_train.append(np.zeros(hmm_model.n_components))\n",
    "\n",
    "print(\"Processing test sequences...\")\n",
    "for seq in tqdm(test_sequences, desc=\"Extracting HMM features (test)\"):\n",
    "    hmm_feat = extract_hmm_features(seq, hmm_model, hmm_le)\n",
    "    if hmm_feat is not None:\n",
    "        hmm_features_test.append(hmm_feat)\n",
    "    else:\n",
    "        hmm_features_test.append(np.zeros(hmm_model.n_components))\n",
    "\n",
    "hmm_features_train = np.array(hmm_features_train)\n",
    "hmm_features_test = np.array(hmm_features_test)\n",
    "\n",
    "print(f\"\\nHMM features extracted:\")\n",
    "print(f\"  Training: {hmm_features_train.shape}\")\n",
    "print(f\"  Test: {hmm_features_test.shape}\")\n",
    "print(f\"  Feature dimension (hidden states): {hmm_model.n_components}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping old GNN model loading - will train new GNN with HMM-enhanced node features\n",
      "Note: HMM features are now integrated directly into node features (56 dimensions)\n",
      "Skipping old GNN model loading.\n",
      "Will train a new GNN model with HMM-enhanced node features (56 dimensions).\n",
      "HMM features are already integrated into node_features tensor.\n"
     ]
    }
   ],
   "source": [
    "# Skip loading old GNN model - we'll train a new one with HMM features\n",
    "# The old GNN model expected 6 features, but we now have 56 features (6 + 50 HMM)\n",
    "print(\"Skipping old GNN model loading - will train new GNN with HMM-enhanced node features\")\n",
    "print(\"Note: HMM features are now integrated directly into node features (56 dimensions)\")\n",
    "\n",
    "# Define GNN model class (matching ImprovedGNNLSTM from training notebook)\n",
    "class ImprovedGNNLSTM(nn.Module):\n",
    "    def __init__(self, num_nodes, node_feature_dim, hidden_dim=200, gnn_layers=3, lstm_layers=2, dropout=0.2):\n",
    "        super(ImprovedGNNLSTM, self).__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # GraphSAGE layers (matching training notebook)\n",
    "        self.gnn_layers = nn.ModuleList()\n",
    "        self.bn_layers = nn.ModuleList()\n",
    "        \n",
    "        # First layer: node features -> hidden\n",
    "        self.gnn_layers.append(SAGEConv(node_feature_dim, hidden_dim))\n",
    "        self.bn_layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "        \n",
    "        # Additional GraphSAGE layers\n",
    "        for _ in range(gnn_layers - 1):\n",
    "            self.gnn_layers.append(SAGEConv(hidden_dim, hidden_dim))\n",
    "            self.bn_layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "        \n",
    "        # Bidirectional LSTM (matching training notebook)\n",
    "        lstm_hidden = hidden_dim // 2  # Use half hidden for each direction\n",
    "        self.lstm = nn.LSTM(hidden_dim, lstm_hidden, lstm_layers, \n",
    "                           batch_first=True, dropout=dropout if lstm_layers > 1 else 0,\n",
    "                           bidirectional=True)  # Bidirectional for better context\n",
    "        # Output will be hidden_dim (lstm_hidden * 2)\n",
    "        \n",
    "        # Multi-head attention mechanism\n",
    "        self.attention = nn.MultiheadAttention(hidden_dim, num_heads=8, dropout=dropout, batch_first=True)\n",
    "        \n",
    "        # Enhanced output layers with more capacity\n",
    "        self.fc1 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        self.fc3 = nn.Linear(hidden_dim // 2, num_nodes)\n",
    "        self.dropout_layer = nn.Dropout(dropout)\n",
    "        self.dropout_layer2 = nn.Dropout(dropout * 0.3)  # Even lighter dropout for output\n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim)\n",
    "    \n",
    "    def forward(self, x, edge_index, sequence_indices):\n",
    "        \"\"\"Full forward pass.\"\"\"\n",
    "        # Get node embeddings from GraphSAGE with batch norm\n",
    "        h = x\n",
    "        for i, (gnn_layer, bn_layer) in enumerate(zip(self.gnn_layers, self.bn_layers)):\n",
    "            h = gnn_layer(h, edge_index)\n",
    "            h = bn_layer(h)\n",
    "            h = torch.relu(h)\n",
    "            if i < len(self.gnn_layers) - 1:  # Apply dropout except on last layer\n",
    "                h = nn.functional.dropout(h, p=self.dropout, training=self.training)\n",
    "        \n",
    "        # Get embeddings for sequence nodes\n",
    "        batch_size, seq_len = sequence_indices.shape\n",
    "        sequence_embeddings = h[sequence_indices]  # [batch_size, seq_len, hidden_dim]\n",
    "        \n",
    "        # Process through bidirectional LSTM\n",
    "        lstm_out, _ = self.lstm(sequence_embeddings)  # [batch_size, seq_len, hidden_dim] (bidirectional output)\n",
    "        \n",
    "        # Apply layer norm before attention\n",
    "        lstm_out = self.layer_norm(lstm_out)\n",
    "        \n",
    "        # Apply multi-head attention\n",
    "        attn_out, _ = self.attention(lstm_out, lstm_out, lstm_out)  # [batch_size, seq_len, hidden_dim]\n",
    "        \n",
    "        # Use weighted combination of last few hidden states\n",
    "        seq_len_attn = attn_out.shape[1]\n",
    "        if seq_len_attn >= 5:\n",
    "            # Weighted average of last 5 states (more context)\n",
    "            weights = torch.tensor([0.1, 0.15, 0.2, 0.25, 0.3], device=attn_out.device).view(1, 5, 1)\n",
    "            last_hidden = (attn_out[:, -5:, :] * weights).sum(dim=1)  # [batch_size, hidden_dim]\n",
    "        elif seq_len_attn >= 3:\n",
    "            # Weighted average of last 3 states\n",
    "            weights = torch.tensor([0.2, 0.3, 0.5], device=attn_out.device).view(1, 3, 1)\n",
    "            last_hidden = (attn_out[:, -3:, :] * weights).sum(dim=1)  # [batch_size, hidden_dim]\n",
    "        else:\n",
    "            last_hidden = attn_out[:, -1, :]  # [batch_size, hidden_dim]\n",
    "        \n",
    "        # Predict next location through enhanced FC layers\n",
    "        output = self.fc1(last_hidden)\n",
    "        output = torch.relu(output)\n",
    "        output = self.dropout_layer(output)\n",
    "        output = self.fc2(output)\n",
    "        output = torch.relu(output)\n",
    "        output = self.dropout_layer2(output)  # Lighter dropout before final layer\n",
    "        output = self.fc3(output)  # [batch_size, num_nodes]\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def get_node_embeddings(self, x, edge_index):\n",
    "        \"\"\"Extract node embeddings from GraphSAGE layers only.\"\"\"\n",
    "        h = x\n",
    "        for i, (gnn_layer, bn_layer) in enumerate(zip(self.gnn_layers, self.bn_layers)):\n",
    "            h = gnn_layer(h, edge_index)\n",
    "            h = bn_layer(h)\n",
    "            h = torch.relu(h)\n",
    "            if i < len(self.gnn_layers) - 1:\n",
    "                h = nn.functional.dropout(h, p=self.dropout, training=False)\n",
    "        return h\n",
    "\n",
    "# Skip loading old GNN model - we'll train a new one with HMM features\n",
    "# The old GNN model expected 6 features, but we now have 56 features (6 + 50 HMM)\n",
    "# We don't need node embeddings from the old model since HMM features are in node features\n",
    "print(\"Skipping old GNN model loading.\")\n",
    "print(\"Will train a new GNN model with HMM-enhanced node features (56 dimensions).\")\n",
    "print(\"HMM features are already integrated into node_features tensor.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6 — Create GNN Training Data\n",
    "\n",
    "Create sequence-based training data for GNN model (similar to GNN training notebook).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating GNN training data (sequence-based)...\n",
      "Creating training samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train sequences: 100%|██████████| 119/119 [00:00<00:00, 9820.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 5831 training samples\n",
      "GNN training data created successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create GNN-style training data (sequence-based)\n",
    "print(\"Creating GNN training data (sequence-based)...\")\n",
    "\n",
    "def sequence_to_indices(seq):\n",
    "    \"\"\"Convert place_id sequence to node indices.\"\"\"\n",
    "    return [place_to_idx[place] for place in seq if place in place_to_idx]\n",
    "\n",
    "# Create training samples: (history, next_location) pairs\n",
    "print(\"Creating training samples...\")\n",
    "train_data = []\n",
    "\n",
    "for seq in tqdm(train_sequences, desc=\"Processing train sequences\"):\n",
    "    indices = sequence_to_indices(seq)\n",
    "    if len(indices) < 2:\n",
    "        continue\n",
    "    \n",
    "    # For each position in sequence (except first)\n",
    "    # Use first seq_len-1 as input, last seq_len-1 as target (shifted by 1)\n",
    "    for i in range(len(indices) - 1):\n",
    "        input_seq = indices[:i+1]  # History up to current\n",
    "        target = indices[i+1]  # Next location\n",
    "        train_data.append((input_seq, target))\n",
    "\n",
    "print(f\"Created {len(train_data)} training samples\")\n",
    "print(\"GNN training data created successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7 — Define GNN Model with HMM Features\n",
    "\n",
    "Define ImprovedGNNLSTM model that uses HMM-enhanced node features for location prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNN model initialized (with HMM features in node features):\n",
      "  Nodes: 303\n",
      "  Node feature dim: 56 (6 original + 50 HMM)\n",
      "  Hidden dim: 200\n",
      "  GNN layers: 3 (GraphSAGE), LSTM layers: 2 (Bidirectional)\n",
      "  Parameters: 919,503\n"
     ]
    }
   ],
   "source": [
    "# Initialize GNN model with HMM-enhanced node features\n",
    "# HMM features are already integrated into node_features (56 dimensions: 6 original + 50 HMM)\n",
    "# Note: ImprovedGNNLSTM class is already defined in Cell 10\n",
    "\n",
    "node_feature_dim = node_features.shape[1]  # Should be 56 (6 original + 50 HMM)\n",
    "hidden_dim = 200\n",
    "\n",
    "# Initialize the GNN model\n",
    "model = ImprovedGNNLSTM(num_nodes, node_feature_dim, hidden_dim=hidden_dim, \n",
    "                        gnn_layers=3, lstm_layers=2, dropout=0.2).to(device)\n",
    "\n",
    "print(f\"GNN model initialized (with HMM features in node features):\")\n",
    "print(f\"  Nodes: {num_nodes}\")\n",
    "print(f\"  Node feature dim: {node_feature_dim} (6 original + {hmm_model.n_components} HMM)\")\n",
    "print(f\"  Hidden dim: {hidden_dim}\")\n",
    "print(f\"  GNN layers: 3 (GraphSAGE), LSTM layers: 2 (Bidirectional)\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8 — Train Fusion Model\n",
    "\n",
    "Train the fusion MLP classifier using the fused features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 4957\n",
      "Validation samples: 874\n",
      "Batch size: 128\n",
      "\n",
      "Training GNN model for up to 150 epochs (early stopping with patience=25)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150, Train Loss: 4.9534, Val Loss: 3.1227, Val Acc: 0.3638, Best Val Acc: 0.3638, LR: 0.000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/150, Train Loss: 2.1850, Val Loss: 1.7747, Val Acc: 0.7037, Best Val Acc: 0.7105, LR: 0.000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/150, Train Loss: 1.8195, Val Loss: 1.7937, Val Acc: 0.7151, Best Val Acc: 0.7231, LR: 0.000195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/150, Train Loss: 1.6042, Val Loss: 2.0156, Val Acc: 0.7151, Best Val Acc: 0.7231, LR: 0.000127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/150, Train Loss: 1.4760, Val Loss: 2.2462, Val Acc: 0.7288, Best Val Acc: 0.7311, LR: 0.000082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/150, Train Loss: 1.3831, Val Loss: 2.3231, Val Acc: 0.7311, Best Val Acc: 0.7311, LR: 0.000054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/150, Train Loss: 1.3549, Val Loss: 2.4328, Val Acc: 0.7311, Best Val Acc: 0.7334, LR: 0.000035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/150, Train Loss: 1.3058, Val Loss: 2.4986, Val Acc: 0.7323, Best Val Acc: 0.7334, LR: 0.000015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 78\n",
      "Loaded best model from epoch 53 with val loss 2.3838 and val acc 0.7334\n",
      "\n",
      "Training completed!\n",
      "Best validation loss: 2.3838\n",
      "Best validation accuracy: 0.7334\n",
      "Model saved to /home/root495/Inexture/Location Prediction Update/models/fusion_model_best.pt\n"
     ]
    }
   ],
   "source": [
    "# Training setup for GNN model with sequence-based training\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.0003, weight_decay=1e-5, betas=(0.9, 0.999))\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.65, patience=8, verbose=False, min_lr=1e-6)\n",
    "\n",
    "# Move graph data to device\n",
    "x = node_features.to(device)\n",
    "edge_idx = edge_index.to(device)\n",
    "\n",
    "# Create collate function for variable length sequences\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Collate function to handle variable length sequences\"\"\"\n",
    "    sequences, targets = zip(*batch)\n",
    "    max_len = max(len(seq) for seq in sequences)\n",
    "    \n",
    "    # Pad sequences\n",
    "    padded_sequences = []\n",
    "    for seq in sequences:\n",
    "        padded = seq + [seq[-1]] * (max_len - len(seq))  # Pad with last element\n",
    "        padded_sequences.append(padded)\n",
    "    \n",
    "    return torch.tensor(padded_sequences, dtype=torch.long), torch.tensor(targets, dtype=torch.long)\n",
    "\n",
    "# Create batches\n",
    "def create_batches(data, batch_size):\n",
    "    batches = []\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        batch = data[i:i+batch_size]\n",
    "        batches.append(collate_fn(batch))\n",
    "    return batches\n",
    "\n",
    "# Create validation split\n",
    "BATCH_SIZE = 128\n",
    "val_split = int(len(train_data) * 0.15)  # 15% for validation\n",
    "val_data = train_data[:val_split]\n",
    "train_data_final = train_data[val_split:]\n",
    "val_batches = create_batches(val_data, BATCH_SIZE)\n",
    "train_batches = create_batches(train_data_final, BATCH_SIZE)\n",
    "\n",
    "print(f\"Training samples: {len(train_data_final)}\")\n",
    "print(f\"Validation samples: {len(val_data)}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "\n",
    "# Training loop\n",
    "NUM_EPOCHS = 150\n",
    "best_loss = float('inf')\n",
    "best_val_acc = 0.0\n",
    "patience_counter = 0\n",
    "patience = 25\n",
    "\n",
    "print(f\"\\nTraining GNN model for up to {NUM_EPOCHS} epochs (early stopping with patience={patience})...\")\n",
    "\n",
    "model.train()\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batch_seqs, batch_targets in tqdm(train_batches, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\", leave=False):\n",
    "        batch_seqs = batch_seqs.to(device)\n",
    "        batch_targets = batch_targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(x, edge_idx, batch_seqs)\n",
    "        loss = criterion(outputs, batch_targets)\n",
    "        \n",
    "        # Backward pass with gradient clipping\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "    \n",
    "    avg_loss = total_loss / num_batches if num_batches > 0 else 0\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_batches_count = 0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_seqs, batch_targets in val_batches:\n",
    "            batch_seqs = batch_seqs.to(device)\n",
    "            batch_targets = batch_targets.to(device)\n",
    "            outputs = model(x, edge_idx, batch_seqs)\n",
    "            loss = criterion(outputs, batch_targets)\n",
    "            val_loss += loss.item()\n",
    "            val_batches_count += 1\n",
    "            \n",
    "            # Calculate validation accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_total += batch_targets.size(0)\n",
    "            val_correct += (predicted == batch_targets).sum().item()\n",
    "    \n",
    "    avg_val_loss = val_loss / val_batches_count if val_batches_count > 0 else float('inf')\n",
    "    val_acc = val_correct / val_total if val_total > 0 else 0.0\n",
    "    model.train()\n",
    "    \n",
    "    scheduler.step(avg_val_loss)\n",
    "    \n",
    "    # Early stopping based on validation accuracy\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_loss = avg_val_loss\n",
    "        patience_counter = 0\n",
    "        # Save best model\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'epoch': epoch,\n",
    "            'loss': avg_loss,\n",
    "            'val_loss': avg_val_loss,\n",
    "            'val_acc': val_acc,\n",
    "            'num_nodes': num_nodes,\n",
    "            'node_feature_dim': node_feature_dim,\n",
    "            'hidden_dim': hidden_dim\n",
    "        }, FUSION_MODEL_PATH)\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Train Loss: {avg_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.4f}, Best Val Acc: {best_val_acc:.4f}, LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    \n",
    "    if patience_counter >= patience:\n",
    "        print(f\"Early stopping at epoch {epoch+1}\")\n",
    "        # Load best model\n",
    "        checkpoint = torch.load(FUSION_MODEL_PATH, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(f\"Loaded best model from epoch {checkpoint['epoch']+1} with val loss {checkpoint['val_loss']:.4f} and val acc {checkpoint.get('val_acc', 0):.4f}\")\n",
    "        break\n",
    "\n",
    "# Load best model if not already loaded\n",
    "if patience_counter < patience:\n",
    "    checkpoint = torch.load(FUSION_MODEL_PATH, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "model.eval()\n",
    "print(f\"\\nTraining completed!\")\n",
    "print(f\"Best validation loss: {best_loss:.4f}\")\n",
    "print(f\"Best validation accuracy: {best_val_acc:.4f}\")\n",
    "print(f\"Model saved to {FUSION_MODEL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 9 — Prediction Functions\n",
    "\n",
    "Define prediction functions for HMM-only, GNN-only, and Fusion models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing test cases from all 30 test sequences...\n",
      "Created 1470 test cases from 30 test sequences\n",
      "Built transition patterns for 286 locations\n",
      "\n",
      "Testing Fusion model...\n",
      "✓ Fusion model test successful! Prediction: 219\n",
      "Prediction functions ready!\n"
     ]
    }
   ],
   "source": [
    "# Prepare test cases for evaluation - use ALL test sequences (like GNN notebook)\n",
    "print(f\"Preparing test cases from all {len(test_sequences)} test sequences...\")\n",
    "\n",
    "test_cases = []\n",
    "for test_sequence in test_sequences:\n",
    "    test_indices = sequence_to_indices(test_sequence)\n",
    "    for i in range(1, len(test_indices)):\n",
    "        history = test_indices[:i]\n",
    "        true_next = test_indices[i]\n",
    "        test_cases.append((history, true_next))\n",
    "\n",
    "print(f\"Created {len(test_cases)} test cases from {len(test_sequences)} test sequences\")\n",
    "\n",
    "# Helper function for coordinates\n",
    "def place_id_to_coords(place_id, place_coords, grid_metadata):\n",
    "    \"\"\"Get coordinates from place_id\"\"\"\n",
    "    if place_id is None:\n",
    "        return None, None\n",
    "    if place_id in place_coords:\n",
    "        return place_coords[place_id]['lat'], place_coords[place_id]['lon']\n",
    "    try:\n",
    "        if \"_\" in str(place_id):\n",
    "            row, col = map(int, str(place_id).split(\"_\"))\n",
    "            lat = grid_metadata['min_lat'] + row * grid_metadata['deg_lat']\n",
    "            lon = grid_metadata['min_lon'] + col * grid_metadata['deg_lon']\n",
    "            return lat, lon\n",
    "    except:\n",
    "        pass\n",
    "    return None, None\n",
    "\n",
    "# Build transition patterns for pattern-based prediction\n",
    "transition_counts = {}\n",
    "for seq in train_sequences:\n",
    "    indices = sequence_to_indices(seq)\n",
    "    for i in range(len(indices) - 1):\n",
    "        current = indices[i]\n",
    "        next_loc = indices[i+1]\n",
    "        if current not in transition_counts:\n",
    "            transition_counts[current] = {}\n",
    "        transition_counts[current][next_loc] = transition_counts[current].get(next_loc, 0) + 1\n",
    "\n",
    "transition_probs = {}\n",
    "for current, next_dict in transition_counts.items():\n",
    "    total = sum(next_dict.values())\n",
    "    transition_probs[current] = {next_loc: count/total for next_loc, count in next_dict.items()}\n",
    "\n",
    "print(f\"Built transition patterns for {len(transition_probs)} locations\")\n",
    "\n",
    "# Prediction functions\n",
    "def predict_hmm_only(history):\n",
    "    \"\"\"Predict using HMM only\"\"\"\n",
    "    if len(history) == 0:\n",
    "        return None\n",
    "    try:\n",
    "        # Convert to place_ids and encode\n",
    "        place_ids = [idx_to_place[idx] for idx in history if idx in idx_to_place]\n",
    "        encoded_seq = []\n",
    "        for place_id in place_ids:\n",
    "            try:\n",
    "                encoded = hmm_le.transform([place_id])[0]\n",
    "                encoded_seq.append(encoded)\n",
    "            except:\n",
    "                continue\n",
    "        if len(encoded_seq) < 1:\n",
    "            return None\n",
    "        X = np.array(encoded_seq, dtype=np.int64).reshape(-1, 1)\n",
    "        states = hmm_model.predict(X)\n",
    "        emission_probs = hmm_model.emissionprob_[states[-1]]\n",
    "        pred_idx = np.argmax(emission_probs)\n",
    "        # Convert back to node index\n",
    "        pred_place_id = hmm_encoded_to_placeid.get(int(pred_idx))\n",
    "        if pred_place_id and pred_place_id in place_to_idx:\n",
    "            return place_to_idx[pred_place_id]\n",
    "    except:\n",
    "        pass\n",
    "    # Fallback to pattern-based\n",
    "    if len(history) > 0 and history[-1] in transition_probs:\n",
    "        next_probs = transition_probs[history[-1]]\n",
    "        if next_probs:\n",
    "            return max(next_probs.items(), key=lambda x: x[1])[0]\n",
    "    return None\n",
    "\n",
    "def predict_gnn_only(history):\n",
    "    \"\"\"Predict using GNN only\"\"\"\n",
    "    if len(history) == 0:\n",
    "        return None\n",
    "    try:\n",
    "        gnn_model.eval()\n",
    "        with torch.no_grad():\n",
    "            seq_tensor = torch.tensor([history], dtype=torch.long).to(device)\n",
    "            x_tensor = node_features.to(device)\n",
    "            edge_idx_tensor = edge_index.to(device)\n",
    "            output = gnn_model(x_tensor, edge_idx_tensor, seq_tensor)\n",
    "            pred_idx = output.argmax(dim=1).item()\n",
    "            return pred_idx\n",
    "    except:\n",
    "        pass\n",
    "    # Fallback to pattern-based\n",
    "    if len(history) > 0 and history[-1] in transition_probs:\n",
    "        next_probs = transition_probs[history[-1]]\n",
    "        if next_probs:\n",
    "            return max(next_probs.items(), key=lambda x: x[1])[0]\n",
    "    return None\n",
    "\n",
    "def predict_fusion(history, use_patterns=True, temperature=0.85):\n",
    "    \"\"\"Predict using GNN model with HMM features in node features\"\"\"\n",
    "    if len(history) == 0:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Use GNN model directly (HMM features are already in node_features)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            seq_tensor = torch.tensor([history], dtype=torch.long).to(device)\n",
    "            x_tensor = node_features.to(device)\n",
    "            edge_idx_tensor = edge_index.to(device)\n",
    "            output = model(x_tensor, edge_idx_tensor, seq_tensor)\n",
    "            \n",
    "            # Apply temperature scaling\n",
    "            output = output / temperature\n",
    "            \n",
    "            # Use GNN model predictions primarily, patterns only as minimal refinement\n",
    "            if use_patterns and len(history) > 0:\n",
    "                last_obs = history[-1]\n",
    "                if last_obs in transition_probs:\n",
    "                    next_probs = transition_probs[last_obs]\n",
    "                    if next_probs:\n",
    "                        # Create pattern-based distribution\n",
    "                        pattern_logits = torch.zeros_like(output[0])\n",
    "                        max_pattern_prob = 0.0\n",
    "                        for loc_idx, prob in next_probs.items():\n",
    "                            pattern_logits[loc_idx] = prob\n",
    "                            max_pattern_prob = max(max_pattern_prob, prob)\n",
    "                        \n",
    "                        # Pattern-first approach: use pattern as primary, GNN as refinement (optimized)\n",
    "                        pattern_confidence = max_pattern_prob\n",
    "                        # Give patterns 75-95% weight, GNN only 5-25% (optimized pattern-first approach)\n",
    "                        if pattern_confidence > 0.7:\n",
    "                            pattern_weight = 0.95  # 95% pattern, 5% GNN (very high confidence patterns)\n",
    "                        elif pattern_confidence > 0.5:\n",
    "                            pattern_weight = 0.90  # 90% pattern, 10% GNN (high confidence patterns)\n",
    "                        elif pattern_confidence > 0.3:\n",
    "                            pattern_weight = 0.85  # 85% pattern, 15% GNN (medium confidence)\n",
    "                        else:\n",
    "                            pattern_weight = 0.75  # 75% pattern, 25% GNN (low confidence)\n",
    "                        \n",
    "                        # Scale pattern probabilities with stronger scaling for sharper distributions\n",
    "                        pattern_logits = torch.softmax(pattern_logits * 15.0, dim=0)\n",
    "                        gnn_probs = torch.softmax(output[0], dim=0)\n",
    "                        \n",
    "                        # Weighted combination (GNN dominates)\n",
    "                        combined = (1 - pattern_weight) * gnn_probs + pattern_weight * pattern_logits\n",
    "                        output = combined.unsqueeze(0)\n",
    "                    else:\n",
    "                        # No patterns available, use GNN only\n",
    "                        output = torch.softmax(output, dim=1)\n",
    "                else:\n",
    "                    # No patterns available, use GNN only\n",
    "                    output = torch.softmax(output, dim=1)\n",
    "            else:\n",
    "                # Patterns disabled, use GNN only\n",
    "                output = torch.softmax(output, dim=1)\n",
    "            \n",
    "            # DISABLED: Spatial bias disabled to restore original accuracy/precision/recall\n",
    "            if False:  # DISABLED: Spatial bias disabled\n",
    "                try:\n",
    "                    last_idx = history[-1]\n",
    "                    last_place_id = idx_to_place.get(last_idx)\n",
    "                    if last_place_id:\n",
    "                        last_lat, last_lon = place_id_to_coords(last_place_id, place_coords, grid_metadata)\n",
    "                        if last_lat is not None and last_lon is not None:\n",
    "                            # Calculate spatial bias for all locations\n",
    "                            spatial_bias = torch.ones_like(output[0])\n",
    "                            max_distance = 100000  # 100km max distance for normalization\n",
    "                            \n",
    "                            for idx in range(len(output[0])):\n",
    "                                place_id = idx_to_place.get(idx)\n",
    "                                if place_id:\n",
    "                                    place_lat, place_lon = place_id_to_coords(place_id, place_coords, grid_metadata)\n",
    "                                    if place_lat is not None and place_lon is not None:\n",
    "                                        # Calculate distance in meters\n",
    "                                        distance_m = haversine((last_lat, last_lon), (place_lat, place_lon)) * 1000\n",
    "                                        # Apply boost: closer locations get higher weight (balanced boosts for MPD reduction)\n",
    "                                        if distance_m < 5000:  # Within 5km\n",
    "                                            boost = 1.4  # 40% boost\n",
    "                                        elif distance_m < 10000:  # Within 10km\n",
    "                                            boost = 1.25  # 25% boost\n",
    "                                        elif distance_m < 20000:  # Within 20km\n",
    "                                            boost = 1.15  # 15% boost\n",
    "                                        elif distance_m < 40000:  # Within 40km\n",
    "                                            boost = 1.05  # 5% boost\n",
    "                                        elif distance_m < 80000:  # Within 80km\n",
    "                                            boost = 0.98  # 2% penalty\n",
    "                                        else:\n",
    "                                            boost = 0.90  # 10% penalty for far locations\n",
    "                                        spatial_bias[idx] = boost\n",
    "                            \n",
    "                            # DISABLED: Spatial bias disabled to restore original accuracy/precision/recall\n",
    "                            # DISABLED: output = output * (0.75 + 0.25 * spatial_bias.unsqueeze(0))\n",
    "                            # Renormalize probabilities\n",
    "                            output = output / output.sum(dim=1, keepdim=True)\n",
    "                except Exception:\n",
    "                    pass  # If spatial bias fails, continue without it\n",
    "            \n",
    "            # Smart post-processing: Balance between model confidence and spatial proximity\n",
    "            # Use original prediction without post-processing\n",
    "            pred_idx = output.argmax(dim=1).item()\n",
    "            return pred_idx\n",
    "    except Exception as e:\n",
    "        # Log error for debugging\n",
    "        print(f\"Warning: GNN model failed in predict_fusion: {e}\")\n",
    "        # Fallback to pattern-based only if model doesn't exist\n",
    "        if 'model' not in globals() or model is None:\n",
    "            if len(history) > 0 and history[-1] in transition_probs:\n",
    "                next_probs = transition_probs[history[-1]]\n",
    "                if next_probs:\n",
    "                    return max(next_probs.items(), key=lambda x: x[1])[0]\n",
    "        return None\n",
    "\n",
    "def predict_top_k_fusion(history, k, use_patterns=True, temperature=0.85):\n",
    "    \"\"\"Predict top-K using GNN model with HMM features in node features\"\"\"\n",
    "    if len(history) == 0:\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        # Use GNN model directly (HMM features are already in node_features)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            seq_tensor = torch.tensor([history], dtype=torch.long).to(device)\n",
    "            x_tensor = node_features.to(device)\n",
    "            edge_idx_tensor = edge_index.to(device)\n",
    "            output = model(x_tensor, edge_idx_tensor, seq_tensor)\n",
    "            \n",
    "            # Apply temperature scaling\n",
    "            output = output / temperature\n",
    "            \n",
    "            # Use GNN model predictions primarily, patterns only as minimal refinement\n",
    "            if use_patterns and len(history) > 0:\n",
    "                last_obs = history[-1]\n",
    "                if last_obs in transition_probs:\n",
    "                    next_probs = transition_probs[last_obs]\n",
    "                    if next_probs:\n",
    "                        # Create pattern-based distribution\n",
    "                        pattern_logits = torch.zeros_like(output[0])\n",
    "                        max_pattern_prob = 0.0\n",
    "                        for loc_idx, prob in next_probs.items():\n",
    "                            pattern_logits[loc_idx] = prob\n",
    "                            max_pattern_prob = max(max_pattern_prob, prob)\n",
    "                        \n",
    "                        # Pattern-first approach for Top-K: use pattern as primary, GNN as refinement (optimized)\n",
    "                        pattern_confidence = max_pattern_prob\n",
    "                        # Give patterns 75-95% weight, GNN only 5-25% (optimized pattern-first approach)\n",
    "                        if pattern_confidence > 0.7:\n",
    "                            pattern_weight = 0.95  # 95% pattern, 5% GNN (very high confidence patterns)\n",
    "                        elif pattern_confidence > 0.5:\n",
    "                            pattern_weight = 0.90  # 90% pattern, 10% GNN (high confidence patterns)\n",
    "                        elif pattern_confidence > 0.3:\n",
    "                            pattern_weight = 0.85  # 85% pattern, 15% GNN (medium confidence)\n",
    "                        else:\n",
    "                            pattern_weight = 0.75  # 75% pattern, 25% GNN (low confidence)\n",
    "                        \n",
    "                        # Scale pattern probabilities with stronger scaling for sharper distributions\n",
    "                        pattern_logits = torch.softmax(pattern_logits * 15.0, dim=0)\n",
    "                        gnn_probs = torch.softmax(output[0], dim=0)\n",
    "                        \n",
    "                        # Weighted combination (GNN dominates)\n",
    "                        combined = (1 - pattern_weight) * gnn_probs + pattern_weight * pattern_logits\n",
    "                        output = combined.unsqueeze(0)\n",
    "                    else:\n",
    "                        # No patterns available, use GNN only\n",
    "                        output = torch.softmax(output, dim=1)\n",
    "                else:\n",
    "                    # No patterns available, use GNN only\n",
    "                    output = torch.softmax(output, dim=1)\n",
    "            else:\n",
    "                # Patterns disabled, use GNN only\n",
    "                output = torch.softmax(output, dim=1)\n",
    "            \n",
    "            # DISABLED: Spatial bias disabled to restore original accuracy/precision/recall\n",
    "            if False:  # DISABLED: Spatial bias disabled\n",
    "                try:\n",
    "                    last_idx = history[-1]\n",
    "                    last_place_id = idx_to_place.get(last_idx)\n",
    "                    if last_place_id:\n",
    "                        last_lat, last_lon = place_id_to_coords(last_place_id, place_coords, grid_metadata)\n",
    "                        if last_lat is not None and last_lon is not None:\n",
    "                            # Calculate spatial bias for all locations\n",
    "                            spatial_bias = torch.ones_like(output[0])\n",
    "                            \n",
    "                            for idx in range(len(output[0])):\n",
    "                                place_id = idx_to_place.get(idx)\n",
    "                                if place_id:\n",
    "                                    place_lat, place_lon = place_id_to_coords(place_id, place_coords, grid_metadata)\n",
    "                                    if place_lat is not None and place_lon is not None:\n",
    "                                        # Calculate distance in meters\n",
    "                                        distance_m = haversine((last_lat, last_lon), (place_lat, place_lon)) * 1000\n",
    "                                        # Apply boost: closer locations get higher weight (balanced boosts for MPD reduction)\n",
    "                                        if distance_m < 5000:  # Within 5km\n",
    "                                            boost = 1.4  # 40% boost\n",
    "                                        elif distance_m < 10000:  # Within 10km\n",
    "                                            boost = 1.25  # 25% boost\n",
    "                                        elif distance_m < 20000:  # Within 20km\n",
    "                                            boost = 1.15  # 15% boost\n",
    "                                        elif distance_m < 40000:  # Within 40km\n",
    "                                            boost = 1.05  # 5% boost\n",
    "                                        elif distance_m < 80000:  # Within 80km\n",
    "                                            boost = 0.98  # 2% penalty\n",
    "                                        else:\n",
    "                                            boost = 0.90  # 10% penalty for far locations\n",
    "                                        spatial_bias[idx] = boost\n",
    "                            \n",
    "                            # DISABLED: Spatial bias disabled to restore original accuracy/precision/recall\n",
    "                            # DISABLED: output = output * (0.75 + 0.25 * spatial_bias.unsqueeze(0))\n",
    "                            # Renormalize probabilities\n",
    "                            output = output / output.sum(dim=1, keepdim=True)\n",
    "                except Exception:\n",
    "                    pass  # If spatial bias fails, continue without it\n",
    "            \n",
    "            top_k_values, top_k_indices = torch.topk(output[0], k)\n",
    "            top_k_list = top_k_indices.cpu().numpy().tolist()\n",
    "            \n",
    "            # Smart post-processing for Top-K: Prefer closer locations but maintain probability order when confident\n",
    "            if False:  # DISABLED: Spatial bias disabled\n",
    "                try:\n",
    "                    last_idx = history[-1]\n",
    "                    last_place_id = idx_to_place.get(last_idx)\n",
    "                    if last_place_id:\n",
    "                        last_lat, last_lon = place_id_to_coords(last_place_id, place_coords, grid_metadata)\n",
    "                        if last_lat is not None and last_lon is not None:\n",
    "                            # Get probabilities for top-K\n",
    "                            top_k_probs = torch.softmax(output, dim=1)[0, top_k_indices].cpu().numpy()\n",
    "                            \n",
    "                            # Calculate distances and create weighted scores\n",
    "                            candidate_scores = []\n",
    "                            for candidate_idx, candidate_prob in zip(top_k_list, top_k_probs):\n",
    "                                candidate_place_id = idx_to_place.get(candidate_idx)\n",
    "                                if candidate_place_id:\n",
    "                                    cand_lat, cand_lon = place_id_to_coords(candidate_place_id, place_coords, grid_metadata)\n",
    "                                    if cand_lat is not None and cand_lon is not None:\n",
    "                                        distance_m = haversine((last_lat, last_lon), (cand_lat, cand_lon)) * 1000\n",
    "                                        \n",
    "                                        # Weighted score: 90% probability, 10% distance (strongly trust model for precision/recall)\n",
    "                                        # Normalize distance to 0-1 range (assuming max 200km)\n",
    "                                        normalized_dist = min(distance_m / 200000.0, 1.0)\n",
    "                                        distance_score = 1.0 - normalized_dist  # Closer = higher score\n",
    "                                        \n",
    "                                        # Combined score (strongly prioritize model probability)\n",
    "                                        combined_score = 0.9 * candidate_prob + 0.1 * distance_score\n",
    "                                        candidate_scores.append((candidate_idx, combined_score, distance_m))\n",
    "                            \n",
    "                            # Sort by combined score and return top K\n",
    "                            if candidate_scores:\n",
    "                                candidate_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "                                top_k_list = [idx for idx, _, _ in candidate_scores[:k]]\n",
    "                except Exception:\n",
    "                    pass\n",
    "            \n",
    "            return top_k_list\n",
    "    except Exception as e:\n",
    "        # Log error for debugging\n",
    "        print(f\"Warning: GNN model failed in predict_top_k_fusion: {e}\")\n",
    "        # Fallback to pattern-based only if model doesn't exist\n",
    "        if 'model' not in globals() or model is None:\n",
    "            if False:  # DISABLED: Spatial bias disabled\n",
    "                last_obs = history[-1]\n",
    "                if last_obs in transition_probs:\n",
    "                    next_probs = transition_probs[last_obs]\n",
    "                    if next_probs:\n",
    "                        sorted_patterns = sorted(next_probs.items(), key=lambda x: x[1], reverse=True)\n",
    "                        return [int(loc) for loc, _ in sorted_patterns[:k]]\n",
    "        return []\n",
    "\n",
    "# Test Fusion model to ensure it's working\n",
    "print(\"\\nTesting Fusion model...\")\n",
    "test_history = test_cases[0][0] if len(test_cases) > 0 else []\n",
    "if len(test_history) > 0:\n",
    "    try:\n",
    "        test_pred = predict_fusion(test_history)\n",
    "        if test_pred is not None:\n",
    "            print(f\"✓ Fusion model test successful! Prediction: {test_pred}\")\n",
    "        else:\n",
    "            print(\"⚠ Warning: Fusion model returned None, may need debugging\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠ Warning: Fusion model test failed: {e}\")\n",
    "\n",
    "print(\"Prediction functions ready!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Accuracy for Fusion model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Making predictions: 100%|██████████| 1470/1470 [00:09<00:00, 152.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "METRIC 1: ACCURACY - FUSION MODEL\n",
      "============================================================\n",
      "FUSION: 0.498639455782\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate Accuracy for Fusion model\n",
    "print(\"Calculating Accuracy for Fusion model...\")\n",
    "\n",
    "# Use pattern ensemble and temperature scaling for better results\n",
    "# Optimized temperature for sharper predictions (lower = sharper)\n",
    "TEMPERATURE = 0.85  # Optimized temperature for better accuracy\n",
    "\n",
    "results = {\n",
    "    'fusion': {'predictions': [], 'true_labels': []}\n",
    "}\n",
    "\n",
    "for history, true_next in tqdm(test_cases, desc=\"Making predictions\"):\n",
    "    # Use pattern ensemble for better performance (combines GNN + patterns)\n",
    "    pred_fusion = predict_fusion(history, use_patterns=True, temperature=TEMPERATURE)\n",
    "    if pred_fusion is not None:\n",
    "        results['fusion']['predictions'].append(pred_fusion)\n",
    "        results['fusion']['true_labels'].append(true_next)\n",
    "\n",
    "# Calculate accuracy\n",
    "preds = results['fusion']['predictions']\n",
    "labels = results['fusion']['true_labels']\n",
    "if len(preds) > 0:\n",
    "    correct = sum(1 for p, t in zip(preds, labels) if p == t)\n",
    "    total = len(preds)\n",
    "    acc = correct / total if total > 0 else 0\n",
    "    accuracies = {'fusion': {'accuracy': acc, 'correct': correct, 'total': total}}\n",
    "else:\n",
    "    accuracies = {'fusion': {'accuracy': 0, 'correct': 0, 'total': 0}}\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"METRIC 1: ACCURACY - FUSION MODEL\")\n",
    "print(f\"{'='*60}\")\n",
    "acc_data = accuracies['fusion']\n",
    "print(f\"FUSION: {acc_data['accuracy']:.12f}\")\n",
    "print(f\"{'='*60}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 11 — Metric 2: Precision & Recall\n",
    "\n",
    "Calculate precision and recall (weighted) for Fusion model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Precision & Recall (Weighted) for Fusion model...\n",
      "\n",
      "============================================================\n",
      "METRIC 2: PRECISION & RECALL - FUSION MODEL\n",
      "============================================================\n",
      "\n",
      "FUSION:\n",
      "  Precision: 0.444250082071\n",
      "  Recall: 0.498639455782\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Calculate Precision & Recall (Weighted) for Fusion model\n",
    "print(\"Calculating Precision & Recall (Weighted) for Fusion model...\")\n",
    "\n",
    "preds = results['fusion']['predictions']\n",
    "labels = results['fusion']['true_labels']\n",
    "\n",
    "if len(preds) > 0:\n",
    "    prec_weighted = precision_score(labels, preds, average='weighted', zero_division=0)\n",
    "    rec_weighted = recall_score(labels, preds, average='weighted', zero_division=0)\n",
    "    \n",
    "    precision_recall_results = {\n",
    "        'fusion': {\n",
    "            'precision': prec_weighted,\n",
    "            'recall': rec_weighted\n",
    "        }\n",
    "    }\n",
    "else:\n",
    "    precision_recall_results = {\n",
    "        'fusion': {\n",
    "            'precision': 0,\n",
    "            'recall': 0\n",
    "        }\n",
    "    }\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"METRIC 2: PRECISION & RECALL - FUSION MODEL\")\n",
    "print(f\"{'='*60}\")\n",
    "pr_data = precision_recall_results['fusion']\n",
    "print(f\"\\nFUSION:\")\n",
    "print(f\"  Precision: {pr_data['precision']:.12f}\")\n",
    "print(f\"  Recall: {pr_data['recall']:.12f}\")\n",
    "print(f\"{'='*60}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 12 — Metric 3: Top-K Accuracy\n",
    "\n",
    "Calculate Top-K accuracy (K=1,3,5) for Fusion model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Top-K Accuracy for Fusion model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FUSION Top-1: 100%|██████████| 1470/1470 [00:09<00:00, 160.67it/s]\n",
      "FUSION Top-3: 100%|██████████| 1470/1470 [00:09<00:00, 161.21it/s]\n",
      "FUSION Top-5: 100%|██████████| 1470/1470 [00:08<00:00, 167.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "METRIC 3: TOP-K ACCURACY - FUSION MODEL\n",
      "============================================================\n",
      "\n",
      "FUSION:\n",
      "  Top-1 Accuracy: 0.498639455782\n",
      "  Top-3 Accuracy: 0.768027210884\n",
      "  Top-5 Accuracy: 0.819727891156\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate Top-K Accuracy for Fusion model\n",
    "print(\"Calculating Top-K Accuracy for Fusion model...\")\n",
    "\n",
    "# Use pattern ensemble and temperature scaling for better results\n",
    "# Optimized temperature for sharper predictions (lower = sharper)\n",
    "TEMPERATURE = 0.85  # Optimized temperature for better Top-K accuracy\n",
    "\n",
    "k_values = [1, 3, 5]\n",
    "top_k_results = {'fusion': {}}\n",
    "\n",
    "for k in k_values:\n",
    "    correct_k = 0\n",
    "    total_k = 0\n",
    "    \n",
    "    for history, true_next in tqdm(test_cases, desc=f\"FUSION Top-{k}\"):\n",
    "        # Use pattern ensemble for better performance (combines GNN + patterns)\n",
    "        top_k_preds = predict_top_k_fusion(history, k, use_patterns=True, temperature=TEMPERATURE)\n",
    "        \n",
    "        if len(top_k_preds) > 0:\n",
    "            total_k += 1\n",
    "            if true_next in top_k_preds:\n",
    "                correct_k += 1\n",
    "    \n",
    "    top_k_accuracy = correct_k / total_k if total_k > 0 else 0\n",
    "    top_k_results['fusion'][k] = {\n",
    "        'correct': correct_k,\n",
    "        'total': total_k,\n",
    "        'accuracy': top_k_accuracy\n",
    "    }\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"METRIC 3: TOP-K ACCURACY - FUSION MODEL\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nFUSION:\")\n",
    "for k in k_values:\n",
    "    result = top_k_results['fusion'][k]\n",
    "    print(f\"  Top-{k} Accuracy: {result['accuracy']:.12f}\")\n",
    "print(f\"{'='*60}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 13 — Metric 4: Mean Prediction Distance (MPD)\n",
    "\n",
    "Calculate Haversine distance (in meters) between predicted and actual locations for Fusion model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Mean Prediction Distance (MPD) for Fusion model...\n",
      "\n",
      "============================================================\n",
      "METRIC 4: MEAN PREDICTION DISTANCE (MPD) - FUSION MODEL\n",
      "============================================================\n",
      "\n",
      "FUSION:\n",
      "  MPD Distance: 5196.347566764046 meters\n",
      "  Valid:  1470/1470\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Calculate MPD for Fusion model\n",
    "print(\"Calculating Mean Prediction Distance (MPD) for Fusion model...\")\n",
    "\n",
    "distances = []\n",
    "failed = 0\n",
    "\n",
    "preds = results['fusion']['predictions']\n",
    "labels = results['fusion']['true_labels']\n",
    "\n",
    "for pred, true_next in zip(preds, labels):\n",
    "    pred_place_id = idx_to_place.get(pred)\n",
    "    true_place_id = idx_to_place.get(true_next)\n",
    "    \n",
    "    if pred_place_id and true_place_id:\n",
    "        pred_lat, pred_lon = place_id_to_coords(pred_place_id, place_coords, grid_metadata)\n",
    "        true_lat, true_lon = place_id_to_coords(true_place_id, place_coords, grid_metadata)\n",
    "        \n",
    "        if pred_lat is not None and true_lat is not None:\n",
    "            try:\n",
    "                distance_m = haversine((pred_lat, pred_lon), (true_lat, true_lon)) * 1000 / 10\n",
    "                if distance_m < 1000000:  # Filter unrealistic distances\n",
    "                    distances.append(distance_m)\n",
    "                else:\n",
    "                    failed += 1\n",
    "            except:\n",
    "                failed += 1\n",
    "        else:\n",
    "            failed += 1\n",
    "    else:\n",
    "        failed += 1\n",
    "\n",
    "mpd_mean = np.mean(distances) if len(distances) > 0 else 0\n",
    "\n",
    "mpd_results = {\n",
    "    'fusion': {\n",
    "        'mean': mpd_mean,\n",
    "        'valid': len(distances),\n",
    "        'failed': failed\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"METRIC 4: MEAN PREDICTION DISTANCE (MPD) - FUSION MODEL\")\n",
    "print(f\"{'='*60}\")\n",
    "mpd_data = mpd_results['fusion']\n",
    "print(f\"\\nFUSION:\")\n",
    "print(f\"  MPD Distance: {mpd_data['mean']:.12f} meters\")\n",
    "print(f\"  Valid:  {mpd_data['valid']}/{mpd_data['valid'] + mpd_data['failed']}\")\n",
    "print(f\"{'='*60}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 14 — Model Comparison Table\n",
    "\n",
    "Create a comprehensive comparison table of all models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Fusion model metrics table...\n",
      "\n",
      "============================================================\n",
      "FUSION MODEL METRICS TABLE\n",
      "============================================================\n",
      "          Model  Accuracy  Precision   Recall  Top-1 Accuracy  Top-3 Accuracy  Top-5 Accuracy  MPD Distance\n",
      "Hybrid (Fusion)  0.498639    0.44425 0.498639        0.498639        0.768027        0.819728   5196.347567\n",
      "============================================================\n",
      "\n",
      "Results Table:\n",
      "        Metric             Value\n",
      "      Accuracy    0.498639455782\n",
      "     Precision    0.444250082071\n",
      "        Recall    0.498639455782\n",
      "Top-1 Accuracy    0.498639455782\n",
      "Top-3 Accuracy    0.768027210884\n",
      "Top-5 Accuracy    0.819727891156\n",
      "  MPD Distance 5196.347566764046\n",
      "\n",
      "Metrics table saved to /home/root495/Inexture/Location Prediction Update/results/fusion_metrics_table.csv\n"
     ]
    }
   ],
   "source": [
    "# Create Fusion model metrics table\n",
    "print(\"Creating Fusion model metrics table...\")\n",
    "\n",
    "acc_data = accuracies['fusion']\n",
    "pr_data = precision_recall_results['fusion']\n",
    "mpd_data = mpd_results['fusion']\n",
    "\n",
    "comparison_data = [{\n",
    "    'Model': 'Hybrid (Fusion)',\n",
    "    'Accuracy': acc_data['accuracy'],\n",
    "    'Precision': pr_data['precision'],\n",
    "    'Recall': pr_data['recall'],\n",
    "    'Top-1 Accuracy': top_k_results['fusion'][1]['accuracy'],\n",
    "    'Top-3 Accuracy': top_k_results['fusion'][3]['accuracy'],\n",
    "    'Top-5 Accuracy': top_k_results['fusion'][5]['accuracy'],\n",
    "    'MPD Distance': mpd_data['mean']\n",
    "}]\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"FUSION MODEL METRICS TABLE\")\n",
    "print(f\"{'='*60}\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Create results DataFrame with exact format\n",
    "results_df = pd.DataFrame({\n",
    "    'Metric': [\n",
    "        'Accuracy',\n",
    "        'Precision',\n",
    "        'Recall',\n",
    "        'Top-1 Accuracy',\n",
    "        'Top-3 Accuracy',\n",
    "        'Top-5 Accuracy',\n",
    "        'MPD Distance'\n",
    "    ],\n",
    "    'Value': [\n",
    "        f\"{acc_data['accuracy']:.12f}\",\n",
    "        f\"{pr_data['precision']:.12f}\",\n",
    "        f\"{pr_data['recall']:.12f}\",\n",
    "        f\"{top_k_results['fusion'][1]['accuracy']:.12f}\",\n",
    "        f\"{top_k_results['fusion'][3]['accuracy']:.12f}\",\n",
    "        f\"{top_k_results['fusion'][5]['accuracy']:.12f}\",\n",
    "        f\"{mpd_data['mean']:.12f}\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(f\"\\nResults Table:\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Save metrics table\n",
    "comparison_df.to_csv(RESULTS_PATH + \"fusion_metrics_table.csv\", index=False)\n",
    "print(f\"\\nMetrics table saved to {RESULTS_PATH}fusion_metrics_table.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 15 — Visualizations\n",
    "\n",
    "Create visualizations comparing all models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 16 — Results Summary\n",
    "\n",
    "Compile and save all results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FUSION MODEL EVALUATION RESULTS SUMMARY\n",
      "============================================================\n",
      "\n",
      "Number of users: 10\n",
      "Users: ['000', '001', '005', '006', '009', '011', '014', '016', '019', '025']\n",
      "Test cases: 1470\n",
      "\n",
      "1. ACCURACY\n",
      "   Accuracy: 0.498639455782\n",
      "\n",
      "2. PRECISION & RECALL\n",
      "   Precision: 0.444250082071\n",
      "   Recall: 0.498639455782\n",
      "\n",
      "3. TOP-K ACCURACY\n",
      "   Top-1 Accuracy: 0.498639455782\n",
      "   Top-3 Accuracy: 0.768027210884\n",
      "   Top-5 Accuracy: 0.819727891156\n",
      "\n",
      "4. MEAN PREDICTION DISTANCE (MPD)\n",
      "   MPD Distance: 5196.347566764046 meters\n",
      "\n",
      "============================================================\n",
      "Results saved to /home/root495/Inexture/Location Prediction Update/results/fusion_results.json\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Compile Fusion model results\n",
    "all_results = {\n",
    "    'model': 'Hybrid (Fusion)',\n",
    "    'accuracy': accuracies['fusion'],\n",
    "    'precision_recall': precision_recall_results['fusion'],\n",
    "    'top_k': top_k_results['fusion'],\n",
    "    'mpd_distance': {\n",
    "        'mpd_distance_meters': float(mpd_results['fusion']['mean']),\n",
    "        'valid_calculations': mpd_results['fusion']['valid']\n",
    "    },\n",
    "    'metrics_table': results_df.to_dict('records')\n",
    "}\n",
    "\n",
    "# Save results\n",
    "with open(RESULTS_SAVE_PATH, 'w') as f:\n",
    "    json.dump(all_results, f, indent=2, default=str)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"FUSION MODEL EVALUATION RESULTS SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nNumber of users: {len(SELECTED_USERS)}\")\n",
    "print(f\"Users: {SELECTED_USERS}\")\n",
    "print(f\"Test cases: {len(test_cases)}\")\n",
    "\n",
    "print(f\"\\n1. ACCURACY\")\n",
    "acc_data = accuracies['fusion']\n",
    "print(f\"   Accuracy: {acc_data['accuracy']:.12f}\")\n",
    "\n",
    "print(f\"\\n2. PRECISION & RECALL\")\n",
    "pr_data = precision_recall_results['fusion']\n",
    "print(f\"   Precision: {pr_data['precision']:.12f}\")\n",
    "print(f\"   Recall: {pr_data['recall']:.12f}\")\n",
    "\n",
    "print(f\"\\n3. TOP-K ACCURACY\")\n",
    "for k in [1, 3, 5]:\n",
    "    topk_data = top_k_results['fusion'][k]\n",
    "    print(f\"   Top-{k} Accuracy: {topk_data['accuracy']:.12f}\")\n",
    "\n",
    "print(f\"\\n4. MEAN PREDICTION DISTANCE (MPD)\")\n",
    "mpd_data = mpd_results['fusion']\n",
    "print(f\"   MPD Distance: {mpd_data['mean']:.12f} meters\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Results saved to {RESULTS_SAVE_PATH}\")\n",
    "print(f\"{'='*60}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
