{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# XGBoost Training on 10 Users' Trajectories\n",
        "\n",
        "This notebook:\n",
        "- Loads 10 users' trajectories (same as HMM/GNN/Fusion/Markov Chain/KNN models)\n",
        "- Removes consecutive duplicates (AAABCDCCABB → ABCDCAB) for each user\n",
        "- Creates sequences of length 50 from all users\n",
        "- Trains an XGBoost multi-class classifier using feature engineering\n",
        "- Evaluates all 4 metrics: Accuracy, Precision & Recall, Top-K Accuracy, MPD\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 1 — Imports & Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "from haversine import haversine\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "from collections import defaultdict, Counter\n",
        "import xgboost as xgb\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds\n",
        "np.random.seed(42)\n",
        "\n",
        "# Paths\n",
        "BASE_PATH = \"/home/root495/Inexture/Location Prediction Update\"\n",
        "PROCESSED_PATH = BASE_PATH + \"/data/processed/\"\n",
        "SEQUENCES_FILE = PROCESSED_PATH + \"place_sequences.json\"\n",
        "GRID_METADATA_FILE = PROCESSED_PATH + \"grid_metadata.json\"\n",
        "CLEANED_WITH_PLACES_FILE = PROCESSED_PATH + \"cleaned_with_places.csv\"\n",
        "OUTPUT_PATH = BASE_PATH + \"/notebooks/\"\n",
        "MODELS_PATH = BASE_PATH + \"/models/\"\n",
        "RESULTS_PATH = BASE_PATH + \"/results/\"\n",
        "MODEL_SAVE_PATH = MODELS_PATH + \"xgboost_model.pkl\"\n",
        "RESULTS_SAVE_PATH = RESULTS_PATH + \"xgboost_results.json\"\n",
        "\n",
        "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
        "os.makedirs(MODELS_PATH, exist_ok=True)\n",
        "os.makedirs(RESULTS_PATH, exist_ok=True)\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2 — Load 10 Users' Trajectories\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading place sequences...\n",
            "Total users available: 54\n",
            "\n",
            "Selected 10 users: ['000', '001', '005', '006', '009', '011', '014', '016', '019', '025']\n",
            "  User 000: 173817 places\n",
            "  User 001: 108561 places\n",
            "  User 005: 108967 places\n",
            "  User 006: 31809 places\n",
            "  User 009: 84573 places\n",
            "  User 011: 90770 places\n",
            "  User 014: 388051 places\n",
            "  User 016: 89208 places\n",
            "  User 019: 47792 places\n",
            "  User 025: 628816 places\n",
            "\n",
            "Total places across all 10 users: 1752364\n"
          ]
        }
      ],
      "source": [
        "# Load place sequences\n",
        "print(\"Loading place sequences...\")\n",
        "with open(SEQUENCES_FILE, 'r') as f:\n",
        "    sequences_dict = json.load(f)\n",
        "\n",
        "print(f\"Total users available: {len(sequences_dict)}\")\n",
        "\n",
        "# Select first 10 users (same as other models)\n",
        "user_ids = list(sequences_dict.keys())\n",
        "NUM_USERS = 10\n",
        "selected_users = user_ids[:NUM_USERS]\n",
        "\n",
        "print(f\"\\nSelected {NUM_USERS} users: {selected_users}\")\n",
        "\n",
        "# Load sequences for all selected users\n",
        "user_sequences = {}\n",
        "total_places = 0\n",
        "for user_id in selected_users:\n",
        "    seq = sequences_dict[user_id]\n",
        "    user_sequences[user_id] = seq\n",
        "    total_places += len(seq)\n",
        "    print(f\"  User {user_id}: {len(seq)} places\")\n",
        "\n",
        "print(f\"\\nTotal places across all {NUM_USERS} users: {total_places}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3 — Preprocess: Remove Consecutive Duplicates\n",
        "\n",
        "Remove consecutive duplicate locations for each user. Example: AAABCDCCABB → ABCDCAB\n",
        "\n",
        "Only consecutive duplicates are removed. If a location appears again later (non-consecutive), it is kept.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing users...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Removing duplicates:  50%|█████     | 5/10 [00:00<00:00, 42.84it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  User 000: 173817 → 795 places (99.5% reduction)\n",
            "  User 001: 108561 → 186 places (99.8% reduction)\n",
            "  User 005: 108967 → 283 places (99.7% reduction)\n",
            "  User 006: 31809 → 103 places (99.7% reduction)\n",
            "  User 009: 84573 → 17 places (100.0% reduction)\n",
            "  User 011: 90770 → 125 places (99.9% reduction)\n",
            "  User 014: 388051 → 766 places (99.8% reduction)\n",
            "  User 016: 89208 → 124 places (99.9% reduction)\n",
            "  User 019: 47792 → 120 places (99.7% reduction)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Removing duplicates: 100%|██████████| 10/10 [00:00<00:00, 29.06it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  User 025: 628816 → 1568 places (99.8% reduction)\n",
            "\n",
            "Summary:\n",
            "  Total original places: 1752364\n",
            "  Total after processing: 4087\n",
            "  Total duplicates removed: 1748277 (99.8%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def remove_consecutive_duplicates(sequence):\n",
        "    \"\"\"\n",
        "    Remove consecutive duplicates from sequence.\n",
        "    Example: [A, A, A, B, C, D, C, C, A, B, B] → [A, B, C, D, C, A, B]\n",
        "    \"\"\"\n",
        "    if len(sequence) == 0:\n",
        "        return sequence\n",
        "    \n",
        "    processed = [sequence[0]]  # Always keep first element\n",
        "    \n",
        "    for i in range(1, len(sequence)):\n",
        "        # Only add if different from previous (not consecutive duplicate)\n",
        "        if sequence[i] != sequence[i-1]:\n",
        "            processed.append(sequence[i])\n",
        "    \n",
        "    return processed\n",
        "\n",
        "# Apply consecutive duplicate removal to each user\n",
        "processed_sequences = {}\n",
        "total_original = 0\n",
        "total_processed = 0\n",
        "\n",
        "print(\"Processing users...\")\n",
        "for user_id in tqdm(selected_users, desc=\"Removing duplicates\"):\n",
        "    original_seq = user_sequences[user_id]\n",
        "    processed_seq = remove_consecutive_duplicates(original_seq)\n",
        "    processed_sequences[user_id] = processed_seq\n",
        "    \n",
        "    original_len = len(original_seq)\n",
        "    processed_len = len(processed_seq)\n",
        "    total_original += original_len\n",
        "    total_processed += processed_len\n",
        "    \n",
        "    reduction = original_len - processed_len\n",
        "    reduction_pct = (reduction / original_len * 100) if original_len > 0 else 0\n",
        "    print(f\"  User {user_id}: {original_len} → {processed_len} places ({reduction_pct:.1f}% reduction)\")\n",
        "\n",
        "print(f\"\\nSummary:\")\n",
        "print(f\"  Total original places: {total_original}\")\n",
        "print(f\"  Total after processing: {total_processed}\")\n",
        "print(f\"  Total duplicates removed: {total_original - total_processed} ({((total_original - total_processed)/total_original*100):.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 4 — Create Sequences of Length 50\n",
        "\n",
        "Split each user's processed sequence into fixed-length chunks of 50 events each.\n",
        "Combine sequences from all users for training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating sequences from all users...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing users: 100%|██████████| 10/10 [00:00<00:00, 16690.43it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  User 000: 30 sequences\n",
            "  User 001: 6 sequences\n",
            "  User 005: 10 sequences\n",
            "  User 006: 3 sequences\n",
            "  User 009: 0 sequences\n",
            "  User 011: 4 sequences\n",
            "  User 014: 29 sequences\n",
            "  User 016: 3 sequences\n",
            "  User 019: 3 sequences\n",
            "  User 025: 61 sequences\n",
            "\n",
            "Total sequences created: 149\n",
            "Total events in sequences: 7450\n",
            "\n",
            "Training sequences: 119\n",
            "Test sequences: 30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Create sequences of fixed length 50\n",
        "SEQUENCE_LENGTH = 50\n",
        "\n",
        "# Use sliding windows for more training data (overlap helps with learning)\n",
        "# Create overlapping sequences with step size of 25 (50% overlap)\n",
        "all_sequences = []\n",
        "step_size = 25  # Overlap of 50%\n",
        "\n",
        "print(\"Creating sequences from all users...\")\n",
        "for user_id in tqdm(selected_users, desc=\"Processing users\"):\n",
        "    processed_seq = processed_sequences[user_id]\n",
        "    user_sequences_list = []\n",
        "    \n",
        "    for i in range(0, len(processed_seq) - SEQUENCE_LENGTH + 1, step_size):\n",
        "        chunk = processed_seq[i:i+SEQUENCE_LENGTH]\n",
        "        if len(chunk) == SEQUENCE_LENGTH:  # Only full-length sequences\n",
        "            user_sequences_list.append(chunk)\n",
        "    \n",
        "    all_sequences.extend(user_sequences_list)\n",
        "    print(f\"  User {user_id}: {len(user_sequences_list)} sequences\")\n",
        "\n",
        "print(f\"\\nTotal sequences created: {len(all_sequences)}\")\n",
        "print(f\"Total events in sequences: {sum(len(s) for s in all_sequences)}\")\n",
        "\n",
        "# Split into train/test (80/20)\n",
        "split_idx = int(len(all_sequences) * 0.8)\n",
        "train_sequences = all_sequences[:split_idx]\n",
        "test_sequences = all_sequences[split_idx:]\n",
        "\n",
        "print(f\"\\nTraining sequences: {len(train_sequences)}\")\n",
        "print(f\"Test sequences: {len(test_sequences)}\")\n",
        "\n",
        "if len(test_sequences) == 0:\n",
        "    # If no test sequences, use last training sequence for testing\n",
        "    test_sequences = [train_sequences[-1]]\n",
        "    train_sequences = train_sequences[:-1]\n",
        "    print(f\"Adjusted: Training={len(train_sequences)}, Test=1 (using last training sequence)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 5 — Encode Sequences\n",
        "\n",
        "Encode place_ids to integers for XGBoost training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encoding sequences...\n",
            "Unique places across all users: 303\n",
            "Encoded 119 training sequences\n",
            "Encoded 30 test sequences\n",
            "Created mapping for 303 place IDs\n"
          ]
        }
      ],
      "source": [
        "# Encode sequences to integers\n",
        "print(\"Encoding sequences...\")\n",
        "le = LabelEncoder()\n",
        "\n",
        "# Flatten all sequences for encoding\n",
        "all_places = [place for seq in train_sequences + test_sequences for place in seq]\n",
        "le.fit(all_places)\n",
        "\n",
        "n_states = len(le.classes_)\n",
        "print(f\"Unique places across all users: {n_states}\")\n",
        "\n",
        "# Encode training sequences\n",
        "train_encoded = []\n",
        "for seq in train_sequences:\n",
        "    encoded = le.transform(seq).tolist()\n",
        "    train_encoded.append(encoded)\n",
        "\n",
        "# Encode test sequences\n",
        "test_encoded = []\n",
        "for seq in test_sequences:\n",
        "    encoded = le.transform(seq).tolist()\n",
        "    test_encoded.append(encoded)\n",
        "\n",
        "print(f\"Encoded {len(train_encoded)} training sequences\")\n",
        "print(f\"Encoded {len(test_encoded)} test sequences\")\n",
        "\n",
        "# Create mapping from encoded ID to original place_id for coordinate lookup\n",
        "encoded_to_placeid = {}\n",
        "for place_id in le.classes_:\n",
        "    encoded_id = le.transform([place_id])[0]\n",
        "    encoded_to_placeid[int(encoded_id)] = place_id\n",
        "\n",
        "print(f\"Created mapping for {len(encoded_to_placeid)} place IDs\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 6 — Feature Engineering\n",
        "\n",
        "Extract features from history sequences to create feature vectors for XGBoost.\n",
        "\n",
        "Features include:\n",
        "- Last N locations (last_1, last_2, ..., last_5)\n",
        "- History length\n",
        "- Location frequency in history (top-K most frequent)\n",
        "- Transition patterns (bigrams)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting up feature engineering...\n",
            "Max history locations: 5\n",
            "Top-K frequent locations: 10\n",
            "\n",
            "Calculating global statistics from training data...\n",
            "Global top-10 most frequent locations: [220, 219, 213, 221, 212]...\n",
            "\n",
            "Feature extraction function defined successfully!\n",
            "Total features: 29 (last_N, history_length, top_freq_loc*K, top_freq_count*K, unique_locations, bigram_loc1, bigram_loc2)\n"
          ]
        }
      ],
      "source": [
        "# Feature Engineering Configuration\n",
        "MAX_HISTORY = 5  # Number of last locations to include as features\n",
        "TOP_K_FREQ = 10  # Number of top frequent locations to track\n",
        "\n",
        "print(\"Setting up feature engineering...\")\n",
        "print(f\"Max history locations: {MAX_HISTORY}\")\n",
        "print(f\"Top-K frequent locations: {TOP_K_FREQ}\")\n",
        "\n",
        "\n",
        "def extract_features(history, max_history=MAX_HISTORY, top_k_freq=TOP_K_FREQ):\n",
        "    \"\"\"\n",
        "    Extract features from history sequence.\n",
        "    \n",
        "    Args:\n",
        "        history: List of encoded states (history sequence)\n",
        "        max_history: Number of last locations to include\n",
        "        top_k_freq: Number of top frequent locations to track\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary of feature values\n",
        "    \"\"\"\n",
        "    features = {}\n",
        "    \n",
        "    # Feature 1-5: Last N locations (padded with -1 if history too short)\n",
        "    for i in range(max_history):\n",
        "        idx = len(history) - (max_history - i)\n",
        "        if idx >= 0:\n",
        "            features[f'last_{i+1}'] = history[idx]\n",
        "        else:\n",
        "            features[f'last_{i+1}'] = -1  # Padding for short histories\n",
        "    \n",
        "    # Feature 6: History length\n",
        "    features['history_length'] = len(history)\n",
        "    \n",
        "    # Feature 7-16: Frequency of top-K most frequent locations in history\n",
        "    if len(history) > 0:\n",
        "        location_counts = Counter(history)\n",
        "        # Get top-K most frequent locations across all training data for consistency\n",
        "        # For now, use top-K from current history\n",
        "        top_freq = location_counts.most_common(top_k_freq)\n",
        "        top_freq_dict = {loc: count for loc, count in top_freq}\n",
        "        \n",
        "        # Calculate most frequent locations in training data (for feature consistency)\n",
        "        # This will be computed from training data later\n",
        "        for i in range(top_k_freq):\n",
        "            if i < len(top_freq):\n",
        "                features[f'top_freq_{i+1}_loc'] = top_freq[i][0]\n",
        "                features[f'top_freq_{i+1}_count'] = top_freq[i][1]\n",
        "            else:\n",
        "                features[f'top_freq_{i+1}_loc'] = -1\n",
        "                features[f'top_freq_{i+1}_count'] = 0\n",
        "    else:\n",
        "        # Empty history\n",
        "        for i in range(top_k_freq):\n",
        "            features[f'top_freq_{i+1}_loc'] = -1\n",
        "            features[f'top_freq_{i+1}_count'] = 0\n",
        "    \n",
        "    # Feature: Number of unique locations in history\n",
        "    features['unique_locations'] = len(set(history)) if len(history) > 0 else 0\n",
        "    \n",
        "    # Feature: Most recent bigram (last 2 locations as a transition feature)\n",
        "    if len(history) >= 2:\n",
        "        features['bigram_loc1'] = history[-2]\n",
        "        features['bigram_loc2'] = history[-1]\n",
        "    else:\n",
        "        features['bigram_loc1'] = -1\n",
        "        features['bigram_loc2'] = history[-1] if len(history) == 1 else -1\n",
        "    \n",
        "    return features\n",
        "\n",
        "\n",
        "# Calculate global top-K most frequent locations from training data\n",
        "print(\"\\nCalculating global statistics from training data...\")\n",
        "all_training_locations = [loc for seq in train_encoded for loc in seq]\n",
        "global_location_counts = Counter(all_training_locations)\n",
        "global_top_freq_locations = [loc for loc, _ in global_location_counts.most_common(TOP_K_FREQ)]\n",
        "\n",
        "print(f\"Global top-{TOP_K_FREQ} most frequent locations: {global_top_freq_locations[:5]}...\")\n",
        "\n",
        "print(\"\\nFeature extraction function defined successfully!\")\n",
        "print(f\"Total features: {MAX_HISTORY + 1 + (TOP_K_FREQ * 2) + 3} (last_N, history_length, top_freq_loc*K, top_freq_count*K, unique_locations, bigram_loc1, bigram_loc2)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 7 — Prepare Training Data\n",
        "\n",
        "Extract features from all training sequences and prepare feature matrix and labels.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting features from training sequences...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing sequences: 100%|██████████| 119/119 [00:00<00:00, 372.94it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Re-encoding labels to consecutive integers for XGBoost...\n",
            "Original label range: 0 to 302\n",
            "Original unique labels: 286\n",
            "Consecutive label range: 0 to 285\n",
            "Consecutive unique labels: 286\n",
            "\n",
            "Training data prepared:\n",
            "  Feature matrix shape: (5831, 29)\n",
            "  Labels shape: (5831,)\n",
            "  Number of features: 29\n",
            "  Number of training samples: 5831\n",
            "  Number of classes (consecutive): 286\n"
          ]
        }
      ],
      "source": [
        "# Prepare training data\n",
        "print(\"Extracting features from training sequences...\")\n",
        "\n",
        "X_train = []\n",
        "y_train = []\n",
        "\n",
        "for seq in tqdm(train_encoded, desc=\"Processing sequences\"):\n",
        "    for i in range(1, len(seq)):\n",
        "        history = seq[:i]\n",
        "        next_location = seq[i]\n",
        "        \n",
        "        # Extract features\n",
        "        features = extract_features(history, max_history=MAX_HISTORY, top_k_freq=TOP_K_FREQ)\n",
        "        \n",
        "        # Convert to list in consistent order\n",
        "        feature_vector = [\n",
        "            features['last_1'], features['last_2'], features['last_3'], \n",
        "            features['last_4'], features['last_5'],\n",
        "            features['history_length'],\n",
        "            features['unique_locations'],\n",
        "            features['bigram_loc1'], features['bigram_loc2']\n",
        "        ]\n",
        "        # Add top-K frequency features\n",
        "        for j in range(TOP_K_FREQ):\n",
        "            feature_vector.append(features[f'top_freq_{j+1}_loc'])\n",
        "            feature_vector.append(features[f'top_freq_{j+1}_count'])\n",
        "        \n",
        "        X_train.append(feature_vector)\n",
        "        y_train.append(next_location)\n",
        "\n",
        "X_train = np.array(X_train, dtype=np.float32)\n",
        "y_train = np.array(y_train, dtype=np.int32)\n",
        "\n",
        "# Re-encode labels to be consecutive integers starting from 0 (required by XGBoost)\n",
        "# Create mapping from original encoded IDs to XGBoost class indices\n",
        "print(\"\\nRe-encoding labels to consecutive integers for XGBoost...\")\n",
        "unique_labels = np.unique(y_train)\n",
        "label_mapping = {orig_label: new_idx for new_idx, orig_label in enumerate(unique_labels)}\n",
        "reverse_label_mapping = {new_idx: orig_label for orig_label, new_idx in label_mapping.items()}\n",
        "\n",
        "# Convert y_train to consecutive indices\n",
        "y_train_consecutive = np.array([label_mapping[label] for label in y_train], dtype=np.int32)\n",
        "\n",
        "print(f\"Original label range: {y_train.min()} to {y_train.max()}\")\n",
        "print(f\"Original unique labels: {len(unique_labels)}\")\n",
        "print(f\"Consecutive label range: {y_train_consecutive.min()} to {y_train_consecutive.max()}\")\n",
        "print(f\"Consecutive unique labels: {len(np.unique(y_train_consecutive))}\")\n",
        "\n",
        "# Use consecutive labels for training\n",
        "y_train = y_train_consecutive\n",
        "\n",
        "print(f\"\\nTraining data prepared:\")\n",
        "print(f\"  Feature matrix shape: {X_train.shape}\")\n",
        "print(f\"  Labels shape: {y_train.shape}\")\n",
        "print(f\"  Number of features: {X_train.shape[1]}\")\n",
        "print(f\"  Number of training samples: {len(X_train)}\")\n",
        "print(f\"  Number of classes (consecutive): {len(label_mapping)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 8 — Train XGBoost Model\n",
        "\n",
        "Train XGBoost multi-class classifier on the feature vectors.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training XGBoost model...\n",
            "Number of unique classes in training data: 286\n",
            "Total possible classes (n_states): 303\n",
            "\n",
            "XGBoost parameters:\n",
            "  objective: multi:softprob\n",
            "  max_depth: 6\n",
            "  learning_rate: 0.1\n",
            "  n_estimators: 100\n",
            "  subsample: 0.8\n",
            "  colsample_bytree: 0.8\n",
            "  random_state: 42\n",
            "  n_jobs: -1\n",
            "  eval_metric: mlogloss\n",
            "\n",
            "Training on 5831 samples...\n",
            "\n",
            "XGBoost model trained successfully!\n",
            "Model has 100 trees\n",
            "Number of classes learned: 286\n"
          ]
        }
      ],
      "source": [
        "# Configure XGBoost parameters\n",
        "print(\"Training XGBoost model...\")\n",
        "\n",
        "# Determine actual number of classes from training data\n",
        "actual_num_classes = len(np.unique(y_train))\n",
        "print(f\"Number of unique classes in training data: {actual_num_classes}\")\n",
        "print(f\"Total possible classes (n_states): {n_states}\")\n",
        "\n",
        "xgb_params = {\n",
        "    'objective': 'multi:softprob',  # Multi-class classification with probabilities\n",
        "    # Note: num_class will be automatically inferred from y_train by XGBoost\n",
        "    'max_depth': 6,\n",
        "    'learning_rate': 0.1,\n",
        "    'n_estimators': 100,\n",
        "    'subsample': 0.8,\n",
        "    'colsample_bytree': 0.8,\n",
        "    'random_state': 42,\n",
        "    'n_jobs': -1,  # Use all available cores\n",
        "    'eval_metric': 'mlogloss'\n",
        "}\n",
        "\n",
        "print(f\"\\nXGBoost parameters:\")\n",
        "for key, value in xgb_params.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "\n",
        "# Train XGBoost model\n",
        "xgb_model = xgb.XGBClassifier(**xgb_params)\n",
        "\n",
        "print(f\"\\nTraining on {len(X_train)} samples...\")\n",
        "xgb_model.fit(\n",
        "    X_train, \n",
        "    y_train,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "print(\"\\nXGBoost model trained successfully!\")\n",
        "print(f\"Model has {xgb_model.n_estimators} trees\")\n",
        "print(f\"Number of classes learned: {len(xgb_model.classes_)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 9 — Prediction Functions\n",
        "\n",
        "Implement prediction functions using XGBoost probability outputs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction functions defined successfully!\n",
            "\n",
            "Function summary:\n",
            "  - predict_next_location(history): Returns single most likely next state\n",
            "  - predict_top_k(history, k=5): Returns top-K most likely next states\n"
          ]
        }
      ],
      "source": [
        "def predict_next_location(history):\n",
        "    \"\"\"\n",
        "    Predict next location using XGBoost model.\n",
        "    \n",
        "    Args:\n",
        "        history: List of encoded states (history sequence)\n",
        "    \n",
        "    Returns:\n",
        "        Predicted next state (encoded integer) or None\n",
        "    \"\"\"\n",
        "    if len(history) == 0:\n",
        "        return None\n",
        "    \n",
        "    # Extract features\n",
        "    features = extract_features(history, max_history=MAX_HISTORY, top_k_freq=TOP_K_FREQ)\n",
        "    \n",
        "    # Convert to feature vector in same order as training\n",
        "    feature_vector = [\n",
        "        features['last_1'], features['last_2'], features['last_3'], \n",
        "        features['last_4'], features['last_5'],\n",
        "        features['history_length'],\n",
        "        features['unique_locations'],\n",
        "        features['bigram_loc1'], features['bigram_loc2']\n",
        "    ]\n",
        "    for j in range(TOP_K_FREQ):\n",
        "        feature_vector.append(features[f'top_freq_{j+1}_loc'])\n",
        "        feature_vector.append(features[f'top_freq_{j+1}_count'])\n",
        "    \n",
        "    feature_array = np.array([feature_vector], dtype=np.float32)\n",
        "    \n",
        "    # Predict class (returns XGBoost class index)\n",
        "    xgb_class_idx = xgb_model.predict(feature_array)[0]\n",
        "    \n",
        "    # Map back to original encoded ID\n",
        "    if xgb_class_idx in reverse_label_mapping:\n",
        "        return int(reverse_label_mapping[xgb_class_idx])\n",
        "    else:\n",
        "        # Fallback: return most frequent label if mapping fails\n",
        "        return None\n",
        "\n",
        "\n",
        "def predict_top_k(history, k=5):\n",
        "    \"\"\"\n",
        "    Get top-K most likely next locations using XGBoost model.\n",
        "    \n",
        "    Args:\n",
        "        history: List of encoded states (history sequence)\n",
        "        k: Number of top predictions to return\n",
        "    \n",
        "    Returns:\n",
        "        List of top-K encoded states sorted by probability\n",
        "    \"\"\"\n",
        "    if len(history) == 0:\n",
        "        return []\n",
        "    \n",
        "    # Extract features\n",
        "    features = extract_features(history, max_history=MAX_HISTORY, top_k_freq=TOP_K_FREQ)\n",
        "    \n",
        "    # Convert to feature vector in same order as training\n",
        "    feature_vector = [\n",
        "        features['last_1'], features['last_2'], features['last_3'], \n",
        "        features['last_4'], features['last_5'],\n",
        "        features['history_length'],\n",
        "        features['unique_locations'],\n",
        "        features['bigram_loc1'], features['bigram_loc2']\n",
        "    ]\n",
        "    for j in range(TOP_K_FREQ):\n",
        "        feature_vector.append(features[f'top_freq_{j+1}_loc'])\n",
        "        feature_vector.append(features[f'top_freq_{j+1}_count'])\n",
        "    \n",
        "    feature_array = np.array([feature_vector], dtype=np.float32)\n",
        "    \n",
        "    # Get probability distribution (returns probabilities for XGBoost class indices)\n",
        "    probabilities = xgb_model.predict_proba(feature_array)[0]\n",
        "    \n",
        "    # Get top-K class indices sorted by probability\n",
        "    top_k_xgb_indices = np.argsort(probabilities)[-k:][::-1]\n",
        "    \n",
        "    # Map back to original encoded IDs\n",
        "    top_k_locations = []\n",
        "    for xgb_idx in top_k_xgb_indices:\n",
        "        if xgb_idx in reverse_label_mapping:\n",
        "            top_k_locations.append(int(reverse_label_mapping[xgb_idx]))\n",
        "    \n",
        "    return top_k_locations\n",
        "\n",
        "\n",
        "print(\"Prediction functions defined successfully!\")\n",
        "print(\"\\nFunction summary:\")\n",
        "print(\"  - predict_next_location(history): Returns single most likely next state\")\n",
        "print(\"  - predict_top_k(history, k=5): Returns top-K most likely next states\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 10 — Save Model\n",
        "\n",
        "Save the trained XGBoost model, encoder, mappings, and feature extraction parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving model...\n",
            "Model saved to /home/root495/Inexture/Location Prediction Update/models/xgboost_model.pkl\n",
            "Model contains:\n",
            "  - XGBoost classifier\n",
            "  - LabelEncoder and mappings\n",
            "  - Feature extraction parameters\n",
            "  - Number of features: 29\n"
          ]
        }
      ],
      "source": [
        "# Save model\n",
        "print(\"Saving model...\")\n",
        "model_data = {\n",
        "    'xgb_model': xgb_model,\n",
        "    'label_encoder': le,\n",
        "    'encoded_to_placeid': encoded_to_placeid,\n",
        "    'label_mapping': label_mapping,  # Original encoded ID -> XGBoost class index\n",
        "    'reverse_label_mapping': reverse_label_mapping,  # XGBoost class index -> Original encoded ID\n",
        "    'n_states': n_states,\n",
        "    'max_history': MAX_HISTORY,\n",
        "    'top_k_freq': TOP_K_FREQ,\n",
        "    'feature_order': ['last_1', 'last_2', 'last_3', 'last_4', 'last_5', \n",
        "                      'history_length', 'unique_locations', 'bigram_loc1', 'bigram_loc2'] + \n",
        "                     [f'top_freq_{j+1}_loc' for j in range(TOP_K_FREQ)] +\n",
        "                     [f'top_freq_{j+1}_count' for j in range(TOP_K_FREQ)]\n",
        "}\n",
        "\n",
        "with open(MODEL_SAVE_PATH, 'wb') as f:\n",
        "    pickle.dump(model_data, f)\n",
        "\n",
        "print(f\"Model saved to {MODEL_SAVE_PATH}\")\n",
        "print(f\"Model contains:\")\n",
        "print(f\"  - XGBoost classifier\")\n",
        "print(f\"  - LabelEncoder and mappings\")\n",
        "print(f\"  - Feature extraction parameters\")\n",
        "print(f\"  - Number of features: {X_train.shape[1]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 11 — Evaluation Setup\n",
        "\n",
        "Prepare test sequences and helper functions for evaluation metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test sequence length: 50 events\n",
            "Created 49 test cases\n",
            "Loaded coordinates for 2073 places\n",
            "Evaluation setup complete!\n"
          ]
        }
      ],
      "source": [
        "# Use first test sequence for evaluation (same as other notebooks for fair comparison)\n",
        "test_sequence = test_encoded[0]\n",
        "print(f\"Test sequence length: {len(test_sequence)} events\")\n",
        "\n",
        "# Create test cases: history -> next location\n",
        "test_cases = []\n",
        "for i in range(1, len(test_sequence)):\n",
        "    history = test_sequence[:i]\n",
        "    true_next = test_sequence[i]\n",
        "    test_cases.append((history, true_next))\n",
        "\n",
        "print(f\"Created {len(test_cases)} test cases\")\n",
        "\n",
        "# Load grid metadata and coordinates for MPD calculation\n",
        "with open(GRID_METADATA_FILE, 'r') as f:\n",
        "    grid_metadata = json.load(f)\n",
        "\n",
        "df_places = pd.read_csv(CLEANED_WITH_PLACES_FILE)\n",
        "place_coords = df_places.groupby('place_id')[['lat', 'lon']].first().to_dict('index')\n",
        "\n",
        "print(f\"Loaded coordinates for {len(place_coords)} places\")\n",
        "\n",
        "# Helper function to get coordinates from place_id\n",
        "def place_id_to_coords(place_id, place_coords, grid_metadata):\n",
        "    \"\"\"Get coordinates from place_id\"\"\"\n",
        "    if place_id is None:\n",
        "        return None, None\n",
        "    \n",
        "    # Try to find in place_coords first\n",
        "    if place_id in place_coords:\n",
        "        return place_coords[place_id]['lat'], place_coords[place_id]['lon']\n",
        "    \n",
        "    # Fallback: calculate from grid if place_id has format \"row_col\"\n",
        "    try:\n",
        "        if \"_\" in str(place_id):\n",
        "            row, col = map(int, str(place_id).split(\"_\"))\n",
        "            lat = grid_metadata['min_lat'] + row * grid_metadata['deg_lat']\n",
        "            lon = grid_metadata['min_lon'] + col * grid_metadata['deg_lon']\n",
        "            return lat, lon\n",
        "    except:\n",
        "        pass\n",
        "    \n",
        "    return None, None\n",
        "\n",
        "print(\"Evaluation setup complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 12 — Metric 1: Accuracy\n",
        "\n",
        "Calculate accuracy: fraction of predictions that exactly match the true next location.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calculating Accuracy...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Making predictions: 100%|██████████| 49/49 [00:00<00:00, 146.09it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Debug - First 5 predictions:\n",
            "  ✗ Pred: 219 (296_2075) | True: 213 (295_2076)\n",
            "  ✗ Pred: 220 (296_2076) | True: 214 (295_2077)\n",
            "  ✓ Pred: 213 (295_2076) | True: 213 (295_2076)\n",
            "  ✓ Pred: 220 (296_2076) | True: 220 (296_2076)\n",
            "  ✓ Pred: 219 (296_2075) | True: 219 (296_2075)\n",
            "\n",
            "============================================================\n",
            "METRIC 1: ACCURACY\n",
            "============================================================\n",
            "Correct predictions: 17\n",
            "Total predictions: 49\n",
            "Accuracy: 0.346938775510\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Calculate Accuracy\n",
        "print(\"Calculating Accuracy...\")\n",
        "predictions = []\n",
        "true_labels = []\n",
        "\n",
        "for history, true_next in tqdm(test_cases, desc=\"Making predictions\"):\n",
        "    pred = predict_next_location(history)\n",
        "    if pred is not None:\n",
        "        predictions.append(pred)\n",
        "        true_labels.append(true_next)\n",
        "\n",
        "# Calculate accuracy\n",
        "if len(predictions) == 0:\n",
        "    print(\"ERROR: No predictions were made!\")\n",
        "    accuracy = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "else:\n",
        "    correct = sum(1 for p, t in zip(predictions, true_labels) if p == t)\n",
        "    total = len(predictions)\n",
        "    accuracy = correct / total if total > 0 else 0\n",
        "    \n",
        "    # Debug: Show first few predictions vs true\n",
        "    print(f\"\\nDebug - First 5 predictions:\")\n",
        "    for i in range(min(5, len(predictions))):\n",
        "        pred_place = encoded_to_placeid.get(predictions[i], \"Unknown\")\n",
        "        true_place = encoded_to_placeid.get(true_labels[i], \"Unknown\")\n",
        "        match = \"✓\" if predictions[i] == true_labels[i] else \"✗\"\n",
        "        print(f\"  {match} Pred: {predictions[i]} ({pred_place[:20]}) | True: {true_labels[i]} ({true_place[:20]})\")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"METRIC 1: ACCURACY\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Correct predictions: {correct}\")\n",
        "print(f\"Total predictions: {total}\")\n",
        "print(f\"Accuracy: {accuracy:.12f}\")\n",
        "print(f\"{'='*60}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 13 — Metric 2: Precision & Recall\n",
        "\n",
        "**Definition**: \n",
        "- **Precision**: How many predicted locations were actually correct, weighted by class frequency\n",
        "- **Recall**: Out of all true next locations, how many you successfully predicted, weighted by class frequency\n",
        "\n",
        "Measuring how trustworthy the model is with visited and predicted locations using weighted averages.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calculating Precision & Recall (Weighted)...\n",
            "\n",
            "============================================================\n",
            "METRIC 2: PRECISION & RECALL\n",
            "============================================================\n",
            "Precision: 0.251753008896\n",
            "Recall: 0.346938775510\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Calculate Precision & Recall (Weighted)\n",
        "print(\"Calculating Precision & Recall (Weighted)...\")\n",
        "\n",
        "if len(predictions) > 0:\n",
        "    precision_weighted = precision_score(true_labels, predictions, average='weighted', zero_division=0)\n",
        "    recall_weighted = recall_score(true_labels, predictions, average='weighted', zero_division=0)\n",
        "else:\n",
        "    precision_weighted = recall_weighted = 0\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"METRIC 2: PRECISION & RECALL\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Precision: {precision_weighted:.12f}\")\n",
        "print(f\"Recall: {recall_weighted:.12f}\")\n",
        "print(f\"{'='*60}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 14 — Metric 3: Top-K Accuracy\n",
        "\n",
        "**Definition**: The true next location is considered correct if it appears in the top K predicted locations.\n",
        "\n",
        "Top-K Accuracy: If the true next position is included in the top-K predictions (K=1, 3, 5).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calculating Top-K Accuracy...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Top-1: 100%|██████████| 49/49 [00:00<00:00, 84.32it/s]\n",
            "Top-3: 100%|██████████| 49/49 [00:00<00:00, 142.29it/s]\n",
            "Top-5: 100%|██████████| 49/49 [00:00<00:00, 162.54it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "METRIC 3: TOP-K ACCURACY\n",
            "============================================================\n",
            "Top-1 Accuracy: 0.346938775510\n",
            "Top-3 Accuracy: 0.448979591837\n",
            "Top-5 Accuracy: 0.551020408163\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Calculate Top-K Accuracy\n",
        "print(\"Calculating Top-K Accuracy...\")\n",
        "\n",
        "k_values = [1, 3, 5]\n",
        "top_k_results = {}\n",
        "\n",
        "for k in k_values:\n",
        "    correct_k = 0\n",
        "    total_k = 0\n",
        "    \n",
        "    for history, true_next in tqdm(test_cases, desc=f\"Top-{k}\"):\n",
        "        top_k_preds = predict_top_k(history, k=k)\n",
        "        if top_k_preds:\n",
        "            total_k += 1\n",
        "            if true_next in top_k_preds:\n",
        "                correct_k += 1\n",
        "    \n",
        "    top_k_accuracy = correct_k / total_k if total_k > 0 else 0\n",
        "    \n",
        "    top_k_results[k] = {\n",
        "        'correct': correct_k,\n",
        "        'total': total_k,\n",
        "        'accuracy': top_k_accuracy\n",
        "    }\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"METRIC 3: TOP-K ACCURACY\")\n",
        "print(f\"{'='*60}\")\n",
        "for k in k_values:\n",
        "    result = top_k_results[k]\n",
        "    print(f\"Top-{k} Accuracy: {result['accuracy']:.12f}\")\n",
        "print(f\"{'='*60}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 15 — Metric 4: Mean Prediction Distance (MPD)\n",
        "\n",
        "**Definition**: Average Haversine distance (in meters) between actual next location and predicted next location.\n",
        "\n",
        "MPD Distance: Mean Prediction Distance — Mean actual distance visited from predicted location of next visit.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calculating Mean Prediction Distance (MPD)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating distances: 100%|██████████| 49/49 [00:01<00:00, 47.62it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "METRIC 4: MEAN PREDICTION DISTANCE (MPD)\n",
            "============================================================\n",
            "MPD Distance: 15229.258501444867 meters\n",
            "Valid distance calculations: 49/49\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Calculate Mean Prediction Distance (MPD)\n",
        "print(\"Calculating Mean Prediction Distance (MPD)...\")\n",
        "\n",
        "distances = []\n",
        "failed_conversions = 0\n",
        "\n",
        "for history, true_next in tqdm(test_cases, desc=\"Calculating distances\"):\n",
        "    pred = predict_next_location(history)\n",
        "    \n",
        "    if pred is not None:\n",
        "        # Convert encoded IDs back to place_ids\n",
        "        pred_place_id = encoded_to_placeid.get(pred)\n",
        "        true_place_id = encoded_to_placeid.get(true_next)\n",
        "        \n",
        "        if pred_place_id and true_place_id:\n",
        "            # Get coordinates\n",
        "            pred_lat, pred_lon = place_id_to_coords(pred_place_id, place_coords, grid_metadata)\n",
        "            true_lat, true_lon = place_id_to_coords(true_place_id, place_coords, grid_metadata)\n",
        "            \n",
        "            if pred_lat is not None and true_lat is not None:\n",
        "                # Calculate haversine distance\n",
        "                try:\n",
        "                    distance_m = haversine((pred_lat, pred_lon), (true_lat, true_lon)) * 1000\n",
        "                    # Filter out unrealistic distances (likely coordinate errors)\n",
        "                    if distance_m < 1000000:  # Less than 1000 km\n",
        "                        distances.append(distance_m)\n",
        "                    else:\n",
        "                        failed_conversions += 1\n",
        "                except:\n",
        "                    failed_conversions += 1\n",
        "            else:\n",
        "                failed_conversions += 1\n",
        "        else:\n",
        "            failed_conversions += 1\n",
        "    else:\n",
        "        failed_conversions += 1\n",
        "\n",
        "if failed_conversions > 0:\n",
        "    print(f\"Warning: {failed_conversions} distance calculations failed or were filtered\")\n",
        "\n",
        "mpd = np.mean(distances) if len(distances) > 0 else 0\n",
        "mpd_median = np.median(distances) if len(distances) > 0 else 0\n",
        "mpd_std = np.std(distances) if len(distances) > 0 else 0\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"METRIC 4: MEAN PREDICTION DISTANCE (MPD)\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"MPD Distance: {mpd:.12f} meters\")\n",
        "print(f\"Valid distance calculations: {len(distances)}/{len(test_cases)}\")\n",
        "print(f\"{'='*60}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "EVALUATION RESULTS SUMMARY\n",
            "============================================================\n",
            "\n",
            "Number of users: 10\n",
            "Users: ['000', '001', '005', '006', '009', '011', '014', '016', '019', '025']\n",
            "Total original places: 1752364\n",
            "After duplicate removal: 4087\n",
            "Training sequences: 119\n",
            "Test sequences: 30\n",
            "\n",
            "1. ACCURACY\n",
            "   Accuracy: 0.346938775510\n",
            "\n",
            "2. PRECISION & RECALL\n",
            "   Precision: 0.251753008896\n",
            "   Recall: 0.346938775510\n",
            "\n",
            "3. TOP-K ACCURACY\n",
            "   Top-1 Accuracy: 0.346938775510\n",
            "   Top-3 Accuracy: 0.448979591837\n",
            "   Top-5 Accuracy: 0.551020408163\n",
            "\n",
            "4. MEAN PREDICTION DISTANCE (MPD)\n",
            "   MPD Distance: 15229.258501444867 meters\n",
            "\n",
            "============================================================\n",
            "\n",
            "Results saved to /home/root495/Inexture/Location Prediction Update/results/xgboost_results.json\n",
            "\n",
            "Results Table:\n",
            "        Metric              Value\n",
            "      Accuracy     0.346938775510\n",
            "     Precision     0.251753008896\n",
            "        Recall     0.346938775510\n",
            "Top-1 Accuracy     0.346938775510\n",
            "Top-3 Accuracy     0.448979591837\n",
            "Top-5 Accuracy     0.551020408163\n",
            "  MPD Distance 15229.258501444867\n"
          ]
        }
      ],
      "source": [
        "# Compile all results\n",
        "results = {\n",
        "    'num_users': NUM_USERS,\n",
        "    'selected_users': selected_users,\n",
        "    'preprocessing': {\n",
        "        'total_original_places': total_original,\n",
        "        'total_after_duplicate_removal': total_processed,\n",
        "        'total_duplicates_removed': total_original - total_processed,\n",
        "        'sequence_length': SEQUENCE_LENGTH,\n",
        "        'total_sequences': len(all_sequences),\n",
        "        'training_sequences': len(train_sequences),\n",
        "        'test_sequences': len(test_sequences)\n",
        "    },\n",
        "    'model': {\n",
        "        'unique_states': n_states,\n",
        "        'model_type': 'xgboost',\n",
        "        'num_features': X_train.shape[1],\n",
        "        'max_history': MAX_HISTORY,\n",
        "        'top_k_freq': TOP_K_FREQ,\n",
        "        'n_estimators': xgb_model.n_estimators,\n",
        "        'max_depth': xgb_model.max_depth\n",
        "    },\n",
        "    'accuracy': {\n",
        "        'value': accuracy,\n",
        "        'correct': correct,\n",
        "        'total': total\n",
        "    },\n",
        "    'precision_recall': {\n",
        "        'precision': float(precision_weighted),\n",
        "        'recall': float(recall_weighted)\n",
        "    },\n",
        "    'top_k_accuracy': {\n",
        "        f'top_{k}_accuracy': float(top_k_results[k]['accuracy']) for k in k_values\n",
        "    },\n",
        "    'mpd_distance': {\n",
        "        'mpd_distance_meters': float(mpd),\n",
        "        'valid_calculations': len(distances)\n",
        "    }\n",
        "}\n",
        "\n",
        "# Display summary\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"EVALUATION RESULTS SUMMARY\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"\\nNumber of users: {NUM_USERS}\")\n",
        "print(f\"Users: {selected_users}\")\n",
        "print(f\"Total original places: {total_original}\")\n",
        "print(f\"After duplicate removal: {total_processed}\")\n",
        "print(f\"Training sequences: {len(train_sequences)}\")\n",
        "print(f\"Test sequences: {len(test_sequences)}\")\n",
        "\n",
        "print(f\"\\n1. ACCURACY\")\n",
        "print(f\"   Accuracy: {accuracy:.12f}\")\n",
        "\n",
        "print(f\"\\n2. PRECISION & RECALL\")\n",
        "print(f\"   Precision: {precision_weighted:.12f}\")\n",
        "print(f\"   Recall: {recall_weighted:.12f}\")\n",
        "\n",
        "print(f\"\\n3. TOP-K ACCURACY\")\n",
        "for k in k_values:\n",
        "    acc = top_k_results[k]['accuracy']\n",
        "    print(f\"   Top-{k} Accuracy: {acc:.12f}\")\n",
        "\n",
        "print(f\"\\n4. MEAN PREDICTION DISTANCE (MPD)\")\n",
        "print(f\"   MPD Distance: {mpd:.12f} meters\")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "\n",
        "# Save results\n",
        "with open(RESULTS_SAVE_PATH, 'w') as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "print(f\"\\nResults saved to {RESULTS_SAVE_PATH}\")\n",
        "\n",
        "# Create results DataFrame\n",
        "results_df = pd.DataFrame({\n",
        "    'Metric': [\n",
        "        'Accuracy',\n",
        "        'Precision',\n",
        "        'Recall',\n",
        "        'Top-1 Accuracy',\n",
        "        'Top-3 Accuracy',\n",
        "        'Top-5 Accuracy',\n",
        "        'MPD Distance'\n",
        "    ],\n",
        "    'Value': [\n",
        "        f\"{accuracy:.12f}\",\n",
        "        f\"{precision_weighted:.12f}\",\n",
        "        f\"{recall_weighted:.12f}\",\n",
        "        f\"{top_k_results[1]['accuracy']:.12f}\",\n",
        "        f\"{top_k_results[3]['accuracy']:.12f}\",\n",
        "        f\"{top_k_results[5]['accuracy']:.12f}\",\n",
        "        f\"{mpd:.12f}\"\n",
        "    ]\n",
        "})\n",
        "\n",
        "print(\"\\nResults Table:\")\n",
        "print(results_df.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added new XGBoost row to models_comparison.csv\n",
            "Updated /home/root495/Inexture/Location Prediction Update/results/models_comparison.csv\n",
            "\n",
            "Updated Models Comparison:\n",
            "         Model       Accuracy      Precision         Recall Top-1 Accuracy Top-3 Accuracy Top-5 Accuracy MPD Distance (meters)\n",
            "           HMM       0.653061       0.605081       0.653061       0.653061       0.897959       0.918367           4364.404451\n",
            "           GNN       0.504762       0.438886       0.504762       0.504762       0.691837       0.787075           3216.861429\n",
            "        Fusion       0.498639        0.44425       0.498639       0.498639       0.768027       0.819728           5196.347567\n",
            "  Markov Chain       0.693878       0.730539       0.693878       0.693878       0.918367       0.918367            3691.02685\n",
            "KNN Trajectory       0.142857       0.020833       0.142857       0.142857       0.367347        0.44898          17011.943967\n",
            "       XGBoost 0.346938775510 0.251753008896 0.346938775510 0.346938775510 0.448979591837 0.551020408163    15229.258501444867\n"
          ]
        }
      ],
      "source": [
        "# Update models_comparison.csv\n",
        "comparison_file = RESULTS_PATH + \"models_comparison.csv\"\n",
        "\n",
        "# Read existing comparison file\n",
        "try:\n",
        "    comparison_df = pd.read_csv(comparison_file)\n",
        "    \n",
        "    # Check if XGBoost row already exists\n",
        "    if 'XGBoost' in comparison_df['Model'].values:\n",
        "        # Update existing row\n",
        "        mask = comparison_df['Model'] == 'XGBoost'\n",
        "        comparison_df.loc[mask, 'Accuracy'] = f\"{accuracy:.12f}\"\n",
        "        comparison_df.loc[mask, 'Precision'] = f\"{precision_weighted:.12f}\"\n",
        "        comparison_df.loc[mask, 'Recall'] = f\"{recall_weighted:.12f}\"\n",
        "        comparison_df.loc[mask, 'Top-1 Accuracy'] = f\"{top_k_results[1]['accuracy']:.12f}\"\n",
        "        comparison_df.loc[mask, 'Top-3 Accuracy'] = f\"{top_k_results[3]['accuracy']:.12f}\"\n",
        "        comparison_df.loc[mask, 'Top-5 Accuracy'] = f\"{top_k_results[5]['accuracy']:.12f}\"\n",
        "        comparison_df.loc[mask, 'MPD Distance (meters)'] = f\"{mpd:.12f}\"\n",
        "        print(\"Updated existing XGBoost row in models_comparison.csv\")\n",
        "    else:\n",
        "        # Add new row\n",
        "        new_row = pd.DataFrame({\n",
        "            'Model': ['XGBoost'],\n",
        "            'Accuracy': [f\"{accuracy:.12f}\"],\n",
        "            'Precision': [f\"{precision_weighted:.12f}\"],\n",
        "            'Recall': [f\"{recall_weighted:.12f}\"],\n",
        "            'Top-1 Accuracy': [f\"{top_k_results[1]['accuracy']:.12f}\"],\n",
        "            'Top-3 Accuracy': [f\"{top_k_results[3]['accuracy']:.12f}\"],\n",
        "            'Top-5 Accuracy': [f\"{top_k_results[5]['accuracy']:.12f}\"],\n",
        "            'MPD Distance (meters)': [f\"{mpd:.12f}\"]\n",
        "        })\n",
        "        comparison_df = pd.concat([comparison_df, new_row], ignore_index=True)\n",
        "        print(\"Added new XGBoost row to models_comparison.csv\")\n",
        "    \n",
        "    # Save updated comparison file\n",
        "    comparison_df.to_csv(comparison_file, index=False)\n",
        "    print(f\"Updated {comparison_file}\")\n",
        "    \n",
        "    # Display updated comparison\n",
        "    print(\"\\nUpdated Models Comparison:\")\n",
        "    print(comparison_df.to_string(index=False))\n",
        "    \n",
        "except FileNotFoundError:\n",
        "    # Create new comparison file if it doesn't exist\n",
        "    comparison_df = pd.DataFrame({\n",
        "        'Model': ['XGBoost'],\n",
        "        'Accuracy': [f\"{accuracy:.12f}\"],\n",
        "        'Precision': [f\"{precision_weighted:.12f}\"],\n",
        "        'Recall': [f\"{recall_weighted:.12f}\"],\n",
        "        'Top-1 Accuracy': [f\"{top_k_results[1]['accuracy']:.12f}\"],\n",
        "        'Top-3 Accuracy': [f\"{top_k_results[3]['accuracy']:.12f}\"],\n",
        "        'Top-5 Accuracy': [f\"{top_k_results[5]['accuracy']:.12f}\"],\n",
        "        'MPD Distance (meters)': [f\"{mpd:.12f}\"]\n",
        "    })\n",
        "    comparison_df.to_csv(comparison_file, index=False)\n",
        "    print(f\"Created new {comparison_file}\")\n",
        "except Exception as e:\n",
        "    print(f\"Warning: Could not update models_comparison.csv: {e}\")\n",
        "    print(\"Results have been saved to JSON file. Please update CSV manually if needed.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
